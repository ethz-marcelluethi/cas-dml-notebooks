{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAJ5G6WyGRMP"
   },
   "source": [
    "<div style=\"background-color: cyan\">\n",
    "This is one of two notebooks about neural networks.\n",
    "One is about neural networks for regression, and the other one about classification. The regression notebook is a bit simpler, and its focus is on the actual neural network and variations of the architecture. The classification notebook is a bit more complex, as you will have to deal with imbalanced data. It also provides more experiments with regularization of neural networks, e.g. using dropout layers.\n",
    "\n",
    "*You don't need to work on both notebooks. Choose the one that best fits your interests.*\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network extension of linear regression\n",
    "\n",
    "In this task, we revisit the ***diamond*** dataset from the previous regression exercise. Earlier, we achieved an $R^2$-score of 0.85 using linear regression to predict the diamond `price`. Now, let's see if we can improve this performance further using a neural network!\n",
    "\n",
    "We will build and train a neural network using the `Keras` library to perform regression on the `price` of the diamonds. This approach will allow us to explore the potential of deep learning for regression tasks and compare its effectiveness to that of traditional regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aY0DMj-8GIZf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, median_absolute_error, root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Seeds\n",
    "We will use random numbers in several places. With the below, we set the seed for the random number generators in order to get reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLc6mKzYJljW"
   },
   "source": [
    "### Data Preparation\n",
    "We use the data provided in the file `diamonds.csv`. First we load the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLax1Nr6GXGL"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('diamonds.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `price` as the target value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define y as the 'price' column\n",
    "y_all = data['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the prediction based on all other values that are numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X by dropping the 'price' column\n",
    "X_all = data.drop('price', axis=1)\n",
    "\n",
    "# Keep only numerical columns in X\n",
    "X_all = X_all.select_dtypes(include=['number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqRv0HYILqKu"
   },
   "source": [
    "**EXERCISE**\n",
    "\n",
    "Write code to split the dataset and scale the features.\n",
    "\n",
    "1. Split the whole dataset into train/val/test sets. Test set is 10% of the whole dataset. Validation set counts 20% of the remaining data (i.e., of the joint training and validation data).\n",
    "2. Standardize the datasets: Use the `StandardScaler` class from the `sklearn.preprocessing` module to standardize features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7sxmdocMQTn"
   },
   "source": [
    "**Hint**:\n",
    "1. Use `random_state=1` to ensure the split is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgRUCzWHNz0S"
   },
   "outputs": [],
   "source": [
    "# 1. Split into train/val/test sets\n",
    "\n",
    "# X_train_val, X_test, y_train, y_test = ...\n",
    "# X_train, X_val, y_train, y_val = t..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDr9_aTXPsaB"
   },
   "outputs": [],
   "source": [
    "# 2. Scale the features\n",
    "\n",
    "# scaler = ...\n",
    "# X_train_scaled = ...\n",
    "# X_val_scaled = ...\n",
    "# X_test_scaled = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRu8CJjqK6E7"
   },
   "source": [
    "## Deep Learning Pipeline\n",
    "Now we are ready to evaluate deep learning models for the price prediction on the `diamonds` dataset. We first define 3 auxiliary functions which take over functionality we will use repeadedly to compare different models. You do not need to understand these functions in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inulLJ79UpxX"
   },
   "outputs": [],
   "source": [
    "def apply_eval_model(model, X, y_true, model_name, do_print=True):\n",
    "    \"\"\"\n",
    "    Function to evaluate a given model on a feature data frame and compute several performance metrics.\n",
    "    The results are returned as a data frame and optionally printed out.\n",
    "\n",
    "    Arguments:\n",
    "    - model: the model to be evaluated\n",
    "    - X: a data frame containing the predictors\n",
    "    - y_true: the true target values, used for the performance assessment\n",
    "    - model_name: name of the model (will be added to the result data frame)\n",
    "    - do_print: optional argument to indicate whether the results should be printed\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    if do_print:\n",
    "        print('r2-Score: ' + str(r2_score(y_true, y_pred)))\n",
    "        print('MSE: ' + str(mean_squared_error(y_true, y_pred)))\n",
    "        print('RMSE: ' + str(root_mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "    if len(model_name)>0:\n",
    "        df = pd.DataFrame({'model_name': model_name,\n",
    "                           'r2_score': r2_score(y_true, y_pred),\n",
    "                           'MSE': mean_squared_error(y_true, y_pred),\n",
    "                           'MAE': mean_absolute_error(y_true, y_pred),\n",
    "                           'RMS': root_mean_squared_error(y_true, y_pred)},\n",
    "                           index=[model_name])\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NMI7yXa5et2"
   },
   "outputs": [],
   "source": [
    "def train_apply_eval_model(model, X_train, y_train, X_val, y_val, model_name='model', num_epochs=20, batch_size=16, do_print=True):\n",
    "    \"\"\"\n",
    "    Train a given model on a training data set, and evaluate it on both the training and validation data.\n",
    "\n",
    "    Arguments:\n",
    "    - model: the model to be evaluated\n",
    "    - X_train: the training predictors\n",
    "    - y_train: the true labels of the training data set\n",
    "    - X_val: the predictors of the validation data set\n",
    "    - y_val: the true labels of the validation data set\n",
    "    - model_name: name of the model (will be added to the result data frame)\n",
    "    - do_print: optional argument to indicate whether the results should be printed\n",
    "    \"\"\"\n",
    "\n",
    "    n_val = X_val.shape[0]\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    if do_print:\n",
    "        print('Evaluation on Training Data:')\n",
    "    res_train = apply_eval_model(model, X_train, y_train, model_name, do_print=do_print)\n",
    "    res_train['dataset'] = 'train'\n",
    "\n",
    "    if n_val>0:\n",
    "        if do_print:\n",
    "            print('\\nEvaluation on Validation Data:')\n",
    "        res_val = apply_eval_model(model, X_val, y_val, model_name, do_print=do_print)\n",
    "        res_val['dataset'] = 'validation'\n",
    "\n",
    "        return history, pd.concat([res_train, res_val])\n",
    "    else:\n",
    "        return history, res_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5titwt6UqqS"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  \"\"\"\n",
    "  Plot model training history.\n",
    "  Args:\n",
    "  - history: tensorflow history object.\n",
    "\n",
    "  Returns:\n",
    "  None\n",
    "  \"\"\"\n",
    "  # Plot loss, precision and recall during training\n",
    "  f, axes = plt.subplots(ncols=2, figsize=(15, 6))\n",
    "\n",
    "  sns.lineplot(x=history.epoch, y=history.history['mean_squared_error'], ax=axes[0], label='Train mse')\n",
    "  sns.lineplot(x=history.epoch, y=history.history['val_mean_squared_error'], ax=axes[0], label='Val mse')\n",
    "  axes[0].set_title('Loss history')\n",
    "  axes[0].set(yscale='log') # Use a log scale on y-axis to show the wide range of values.\n",
    "  axes[0].set(xlabel='Epoch', ylabel='Loss')\n",
    "\n",
    "  sns.lineplot(x=history.epoch, y=history.history['r2_score'], ax=axes[1], label='Train r2_score')\n",
    "  sns.lineplot(x=history.epoch, y=history.history['val_r2_score'], ax=axes[1], label='Val r2_score')\n",
    "  axes[1].set_title('r2_score history')\n",
    "  axes[1].set(xlabel='Epoch', ylabel='Precision')\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJX89Rw-QA34"
   },
   "source": [
    "### Define Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to define the model architecture. We start with the model itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "MVo1Ays-Pzct",
    "outputId": "b5b15da6-1613-4633-88dc-a1aec3af64e3"
   },
   "outputs": [],
   "source": [
    "# Define deep learning model\n",
    "model_1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the optimizer - these are some standard settings we will not consider in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to compile the model in order to combine necessary components together. You must compile it before starting the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mean_squared_error', 'r2_score']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get a summary of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6DuEGpToVE3"
   },
   "source": [
    "**EXERCISE**\n",
    "\n",
    "1. How many layers are there in this architecture?\n",
    "2. Looking at the summary table, explain the number of parameters related to the first hidden layer. *Hint: Don't forget the bias terms*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBXa7qP5QC4C"
   },
   "source": [
    "### Train the model\n",
    "Now we do the actual training of the model. We will do 20 epochs. For the training, we will use our helper function `train_apply_eval_model`, which returns both the training history and a result overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cpwjCVWLUdtU",
    "outputId": "d5c009f0-51dc-49b9-b387-50696d2df196"
   },
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "num_epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "# Train the model\n",
    "history_1, base_model_result = train_apply_eval_model(model_1, X_train_scaled, y_train, X_val_scaled, y_val, \n",
    "                                                      model_name='base', num_epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaBLRiPgQMO6"
   },
   "source": [
    "### Evaluate the model\n",
    "We will first look at the summary results of this first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** What do you observe here? Please describe the performance of the model on the training and validation dataset. **Hint**: We will compare the performance of the neural networks with the results obtained using the \"classical\" regression methods later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also look at the evolution of the performance as the training progresses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "OAK8N9_rU9t4",
    "outputId": "08ec80ea-9e8c-4992-eb02-980cdc033a9b"
   },
   "outputs": [],
   "source": [
    "plot_history(history_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzdGWainpTcG"
   },
   "source": [
    "**EXERCISE**:\n",
    "\n",
    "1) Does the training loss decrease steadily over epochs?\n",
    "\n",
    "2) How does the validation loss behave compared to the training loss?\n",
    "\n",
    "3) Using the answers of the 2 first questions, can you say infere something about model generalisation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKtCmTzCK86N"
   },
   "source": [
    "## Model Variations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1x7Gfam9lhzY"
   },
   "source": [
    "### Wider Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaV_hLT0ll_t"
   },
   "source": [
    "**EXERCISE**:\n",
    "\n",
    "Define a wider neural network\n",
    "\n",
    "- The model consists of a input layer of 64 neurons, two hidden layers of 128 neurons, and an output layer of 1 neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZqBHu6fnGfE"
   },
   "outputs": [],
   "source": [
    "# Define deep learning model\n",
    "# model_2 = ...\n",
    "\n",
    "# Define the optimizer\n",
    "# learning_rate = 0.001\n",
    "# optimizer = ...\n",
    "\n",
    "# Compile the model. This means to combine necessary components together. You must compile it before start training.\n",
    "# model_2.compile(\n",
    "#     optimizer=...,\n",
    "#     loss=...,\n",
    "#     metrics=...\n",
    "#     )\n",
    "\n",
    "# Print the model info\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6C_eP4PPnb08",
    "outputId": "2f81490e-cbbe-4c50-a844-86970c4005e4"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history_2, wider_model_result = train_apply_eval_model(model_2, X_train_scaled, y_train, X_val_scaled, y_val, \n",
    "                                                       model_name='wider', num_epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "7HlzrzfLn6cX",
    "outputId": "d8a39e98-0414-40d3-be7f-6deb95605bd1"
   },
   "outputs": [],
   "source": [
    "plot_history(history_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJ-tkseqoA1l"
   },
   "source": [
    "### Deeper Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tql2V7O9RF7m"
   },
   "source": [
    "**EXERCISE**:\n",
    "\n",
    "Define a deeper neural network\n",
    "\n",
    "- The model consists of a input layer of 32 neurons, four hidden layers of 64 neurons, and an output layer of 1 neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ul0YOYGnoD4b"
   },
   "outputs": [],
   "source": [
    "# Define deep learning model\n",
    "# model_3 = ...\n",
    "\n",
    "# Define the optimizer\n",
    "# learning_rate = 0.001\n",
    "# optimizer = ...\n",
    "\n",
    "# Compile the model. This means to combine necessary components together. You must compile it before start training.\n",
    "# model_3.compile(\n",
    "#     optimizer=...,\n",
    "#     loss=...,\n",
    "#     metrics=...\n",
    "#     )\n",
    "\n",
    "# Print the model info\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mqJ_k5uKph9f",
    "outputId": "aeb4310b-daee-4f1b-9633-f10aee955bf1"
   },
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "num_epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "# Train the model\n",
    "history_3, deeper_model_result = train_apply_eval_model(model_3, X_train_scaled, y_train, X_val_scaled, y_val, model_name='deeper', num_epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "REYfvfm-pjXf",
    "outputId": "a32d814e-d05f-45fa-d75f-c733462daf50"
   },
   "outputs": [],
   "source": [
    "plot_history(history_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXnX0CZMLcfU"
   },
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ec3d3Y0BnSQe"
   },
   "outputs": [],
   "source": [
    "all_results = pd.concat([base_model_result, wider_model_result, deeper_model_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "RmMYgnBDvOow",
    "outputId": "b22e95f1-b959-4cbb-ac91-cb696cc5a67f"
   },
   "outputs": [],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "06n-XU4d_WzQ",
    "outputId": "03f27bb0-16ff-47d5-b4a3-958c497a23ae"
   },
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "metrics =['r2_score', 'RMS']\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    sns.barplot(data=all_results, y='model_name', x=metric, hue='dataset', ax=axs[i])\n",
    "    axs[i].legend(loc='lower center')\n",
    "    axs[i].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTdOP-96hkfl"
   },
   "source": [
    "To get a more detailed results, consider the varying more the layers and sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nabpE2Gkfs3Z",
    "outputId": "dc1845c1-5964-42d4-c906-14079d66b546",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_neurons = [32, 64, 128]\n",
    "n_layers = [1, 2, 4]\n",
    "results = []\n",
    "\n",
    "\n",
    "for n_layer in n_layers:\n",
    "    for n_neuron in n_neurons:\n",
    "        print('\\n****************************')\n",
    "\n",
    "        # define network:\n",
    "        model = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "             tf.keras.layers.Dense(n_neuron, activation='relu')] + \n",
    "            [tf.keras.layers.Dense(n_neuron*2, activation='relu') for _ in range(n_layer)] + \n",
    "            [tf.keras.layers.Dense(1)]\n",
    "        )\n",
    "\n",
    "        # define optimizer:\n",
    "        learning_rate = 0.001\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "        # compile model\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mean_squared_error',\n",
    "            metrics=['mean_squared_error', 'r2_score']\n",
    "        )\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        # train and evaluate the model\n",
    "        history, result = train_apply_eval_model(model, X_train_scaled, y_train, X_val_scaled, y_val, \n",
    "                                                 model_name=f\"{n_layer+2}_layers-{n_neuron}_size\",\n",
    "                                                 num_epochs=5, batch_size=batch_size)\n",
    "        \n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "nkBjeB1Ihflh",
    "outputId": "ad7fc595-4593-4dbe-d069-3391f3cb8f1b"
   },
   "outputs": [],
   "source": [
    "results_df = pd.concat(results)\n",
    "\n",
    "# Plotting the results\n",
    "metrics =['r2_score', 'RMS']\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    sns.barplot(data=results_df, y='model_name', x=metric, hue='dataset', ax=axs[i])\n",
    "    axs[i].legend(loc='lower center')\n",
    "    axs[i].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWfRM_UEN0ho"
   },
   "source": [
    "Let's load the results from Project 1 and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "-ghLg6VvP3v5",
    "outputId": "c7cf2805-5394-4e68-ee09-0fd8a2b8c192"
   },
   "outputs": [],
   "source": [
    "project1_results = pd.read_csv('regression_results.csv')\n",
    "project1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We join the two results dataframes and plot both the r2 score and the root mean squared error of all models we considered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat([project1_results, results_df]).reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=all_results, y='model_name', x='r2_score', hue='dataset')\n",
    "plt.grid()\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=all_results, y='model_name', x='RMS', hue='dataset')\n",
    "plt.grid()\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=all_results, y='model_name', x='MAE', hue='dataset')\n",
    "plt.grid()\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7twtOrLNr9b"
   },
   "source": [
    "**EXERCISE**\n",
    "\n",
    "- Compare the results of deep learning models with the results of traditional regression models.\n",
    "- Do deep learning models outperform traditional regressors in terms of MAE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvD0UdBq2sWS"
   },
   "source": [
    "## Effect of Dataset Size\n",
    "The following cell trains a rather small neural network on a varying number of training data. For each value of `training_sample_counts`, the given number of training data points is sampled randomly from the full training data set 10 times. The model is then trained on this randomly sampled training data set, and evaluated on the full validation dataset.\n",
    "\n",
    "As the training can take quite some time, we have stored the results in a file `all_model_results.csv` that is available to you. If you want to run the trainings on your own, set `run_from_scratch = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_from_scratch = False\n",
    "\n",
    "if run_from_scratch:\n",
    "    n_train_all = X_train_scaled.shape[0]\n",
    "    training_sample_counts = [ 2, 5, 10, 100, 1000, 10000 ]\n",
    "    \n",
    "    # define model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    learning_rate = 0.0001\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    all_model_results = pd.DataFrame()\n",
    "    np.random.seed(123)\n",
    "    \n",
    "    for n_train in training_sample_counts:\n",
    "        # randomly sample the required number of indices\n",
    "        print('Using ' + str(n_train) + ' samples for training')\n",
    "        for iter in range(10):\n",
    "            print('iteration', iter)\n",
    "            my_train_idx = np.random.choice(range(n_train_all), size=n_train, replace=False)\n",
    "        \n",
    "            # select the respective training data points from the training data set (X and y):\n",
    "            my_X_train = X_train_scaled[my_train_idx]\n",
    "            my_y_train = y_train[my_train_idx]\n",
    "        \n",
    "            model.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss='mean_squared_error',\n",
    "                metrics=['mean_squared_error', 'r2_score']\n",
    "                )\n",
    "        \n",
    "            # train and evaluate model performance\n",
    "            _, all_features_results = train_apply_eval_model(model, my_X_train, my_y_train, X_val_scaled, y_val,\n",
    "                                                             model_name='NN', num_epochs=50, do_print=False)\n",
    "            all_features_results['n_train'] = n_train\n",
    "            all_features_results['iter'] = iter\n",
    "        \n",
    "            # add the result data frame to the overall data frame\n",
    "            all_model_results = pd.concat([all_model_results, all_features_results])\n",
    "    \n",
    "    all_model_results.to_csv('all_model_results.csv')\n",
    "else:\n",
    "    all_model_results = pd.read_csv('all_model_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell plots the results in the same way you have seen in the lecture. The bars represent the average performance (in the given performance metric) on the dataset given by the color; the black bars (errorbars) represent the standard deviation over the 10 sampled datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'RMS'\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "sns.barplot(data=all_model_results_small, errorbar='sd', x='n_train', y=metric, hue='dataset')\n",
    "plt.tick_params(axis='x', rotation=45)\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.title('Model Performance depending on Training Data Size', fontsize=\"x-large\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:**\n",
    "Describe and interprete the above results. In particular, interprete the average performance on the training and validation datasets, as well as their standard deviation, as the size of the training data varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
