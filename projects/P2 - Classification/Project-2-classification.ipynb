{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiIordFhvXlo"
   },
   "source": [
    "# CAS DML Course Project 2: Classification\n",
    "\n",
    "\n",
    "This project helps you get hands-on experience with the classification techniques your learned in the lecture, using two real-world datasets. The simpler dataset is on credit card fraud detection, and the more challenging dataset is on predicting bankruptcy. \n",
    "\n",
    "This notebook not only contains code but also some explanations that help you deepen your understanding. Most important are, however, the exercises. They are designed to help you apply what you have learned and to reflect on the results. All exercises are marked by **EXERCISE**. Some exercises will ask you reflect on results or experiments, while others will ask you to code something. Coding exercises usually come with a cell as the one below.\n",
    "\n",
    "\n",
    "ðŸ“‘ **A note on python**\n",
    "We are aware that you might not be very familiar with Python at this stage in your learning journey. You are not expected to understand all the details of the code. Rather, you should be able to understand what the code in a cell is achieving and how you can influence its behavior. We have designed all the exercises that involve programming, such that you should be able to solve them by just copying and slightly adapting code that you have seen before in one of the notebooks. \n",
    "\n",
    "ðŸ“‘ **A note on the datasets**\n",
    "\n",
    "The credit card fraud dataset is called `card_data.csv` and the bankruptcy dataset is called `bank_data.csv`. Both datasets are stored in the CSV format.\n",
    "\n",
    "The original datasets can be found here:\n",
    "* https://www.kaggle.com/datasets/dhanushnarayananr/credit-card-fraud/data\n",
    "* https://www.kaggle.com/datasets/fedesoriano/company-bankruptcy-prediction/data\n",
    "\n",
    "The datasets have been cleaned for you and contain only numerical features, such that you can focus on the actual classification task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngCv79de3cgA"
   },
   "source": [
    "## Dataset exploration and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iH-56ybGOOWL"
   },
   "source": [
    "In this section we will explore the dataset. We will compute some statistics but also visually inspect the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roOAGjxARksi"
   },
   "source": [
    "**Dataset info**\n",
    "\n",
    "We use the *Taiwanese Bankruptcy Prediction* dataset for Exploratory Data Analysis. The data was collected from the Taiwan Economic Journal between the years of 1999 to 2009. Company bankruptcy was defined based on the business regulations of the Taiwan Stock Exchange. The values are collected from the latest financial report from the companies. The dataset has been preprocessed, eliminating any missing values or outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8OlB4QeX5EL"
   },
   "source": [
    "### Data statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Imk6oaoI-H-_"
   },
   "source": [
    "Lets' first get to know the basics of the dataset. We start by importing necessary modules then load the dataset downloaded previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLVXsFQDvK5i"
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd  # For data manipulation\n",
    "import seaborn as sns # For data visulization\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "import numpy as np  # For numerical operations\n",
    "\n",
    "bank_data = pd.read_csv(\"bank_data.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmjT2s1fc-Vg"
   },
   "source": [
    "Usually we print a few samples and info of the dataset to get some intuition about what the dataset is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "N8b4gm2-_5Dn",
    "outputId": "6bf63190-70f1-4778-8cec-5e6af6db3235"
   },
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the dataset\n",
    "bank_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtDBnCeyhSYw"
   },
   "source": [
    "We can also print basic information about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VIxDY6j_hEn2",
    "outputId": "97a4530e-9f4a-4870-acef-c9b3f44326ce"
   },
   "outputs": [],
   "source": [
    "# Display basic information about the dataset. This will give the column names and the corresponding data type.\n",
    "bank_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItWdvnBHhl9K"
   },
   "source": [
    "From the output of the previous two cells, we know that the dataset contains 96 columns and 6270 rows. The column \"Bankrupt?\" is what we want to predict. So the input feature would be a 95-dimensional vector.  \n",
    "\n",
    "To know more about the dataset numerically, we need some numbers that quantitatively describe the dataset. These numbers are called statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "bqYIDIvmB-Gr",
    "outputId": "b0f21b12-1f4f-4604-df27-cf3ac9ba1cbf"
   },
   "outputs": [],
   "source": [
    "# Display summary statistics of the dataset\n",
    "bank_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the above command gives us the *count*, the *mean*, the *standard deviation*, the *minimum*, the *25th percentile*, the *50th percentile*, the *75th percentile*, and the *maximum* of each column.\n",
    "\n",
    "The count is the number of samples in the dataset. The mean is the average of all samples. The standard deviation is a measure of the amount of variation of a random variable expected about its mean. The minimum and maximum are the smallest and largest values in the dataset. The 25th, 50th, and 75th percentiles are the values below which a given percentage of observations in a group of observations fall.\n",
    "\n",
    "We could, of course, easily compute these statistics for each column separately.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "trAzZItGWO4E",
    "outputId": "5520490b-6f96-4a14-8cc7-ec1c20eadbf2"
   },
   "outputs": [],
   "source": [
    "# Calculate the mean and the standard deviation of \"Tax rate (A)\". Compare them with the results from the previous block.\n",
    "mean = bank_data['Tax rate (A)'].mean()\n",
    "std = bank_data['Tax rate (A)'].std()\n",
    "\n",
    "# Print the mean and the standard deviation\n",
    "print(\"Average tax rate:\", mean)\n",
    "print(\"Standard deviation of tax rate:\", std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**\n",
    "\n",
    "- Compute the mean and standard deviation of the column `Operating Profit Rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4_5r0EYeW84A",
    "outputId": "814ee993-b370-498a-ddcd-54280061e541"
   },
   "outputs": [],
   "source": [
    "\n",
    "mean = ...\n",
    "std = ...\n",
    "\n",
    "# Print the mean and the standard deviation\n",
    "print(\"Average:\", mean)\n",
    "print(\"Standard Deviation:\", std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might also be interesting to compute correlations between two columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**\n",
    "\n",
    "- Calculate the correlation between \"Operating Profit Rate\" and \"Operating profit per person\" \n",
    "    - *Hint*: to calculate correlation between \"A\" and \"B\", use df[\"A\"].corr(df[\"B\"])\n",
    "- How do you interpret this correlation value? Indicate whether the two features are positively, negatively or not correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr = ...\n",
    "\n",
    "# Print the correlation\n",
    "print(\"Correlation:\", corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer to the second question*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxRNVQeZYHO7"
   },
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctMv4Q5afInW"
   },
   "source": [
    "We first visualize the target distribution, i.e., how many companies are bankrupt with respect to all the companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "YwNMvRCHjBqK",
    "outputId": "a9896e9a-e8ed-4f60-ad89-902cbf0822fa"
   },
   "outputs": [],
   "source": [
    "plt.pie(bank_data.value_counts(\"Bankrupt?\"), labels = [\"Not bankrupt\", \"Bankrupt\"], autopct='%1.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CP2D2KZYfiRh"
   },
   "source": [
    "The pie plot shows that only a small fraction of companies are bankrupt. This is important information, as this means that the classifier has to learn to identify bankruptcy cases from a small number of samples. We will address this problem later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's also visualize some features here, in order to get an impression of the distribution of values for each feature. We first plot a histogram for some features, then select the ones that seem interesting for more detailed plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3SARmQ6IHBe7",
    "outputId": "b3dc4707-bfec-4354-ab9d-39aaf27e6a08"
   },
   "outputs": [],
   "source": [
    "# Plot histograms for the first 30 features\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "rows, cols = 10, 3\n",
    "for idx in range(rows*cols):\n",
    "    ax = fig.add_subplot(rows, cols, idx+1)\n",
    "    ax.grid(alpha = 0.7, axis =\"both\")\n",
    "    sns.histplot(x = bank_data.drop(columns=[\"Bankrupt?\"]).columns[idx], fill = True, color =\"#3386FF\", data = bank_data, bins=30)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4SG6orHhB33"
   },
   "source": [
    "From the histograms we can see most features are highly concentrated (eg. Revenue Per Share), while some might have multiple peaks(Total Asset Growth Rate). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**\n",
    "\n",
    "- Which features are more useful in a classifiaction task? The highly concentrated ones with small variance or the ones that spread out over a wide range of values or even have multiple peaks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer: Your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can also visualize the correlation between all pairs of features by plotting the correlation matrix. It can be a bit overwhelming, but don't worry. We just use it to get an overview of the data and see if there's obvious data anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0EYPQkw9gkJo",
    "outputId": "d952c416-aff1-49ea-9a3b-8d115afae557"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(30, 25))\n",
    "correlation = bank_data.corr()\n",
    "mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(correlation, mask=mask, cmap=cmap, vmax=1, center=0,\n",
    "      square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16N0_V4yi41T"
   },
   "source": [
    "We see that \"Net Income Flag\" has invalid correlation values with every feature. It is probably because it has zero standard deviation, which means it is constant. Let's check:\n",
    "\n",
    "**EXERCISE**\n",
    "\n",
    "- Check if \"Net Income Flag\" has zero standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S387XVowjI7C",
    "outputId": "a7677272-30f7-438a-b94a-94752c5a2bd9"
   },
   "outputs": [],
   "source": [
    "std_net_income_flag = ... \n",
    "\n",
    "print(std_net_income_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ItvAyMtrH3I"
   },
   "outputs": [],
   "source": [
    "# Remove \"Net Income Flag\" feature\n",
    "bank_data = bank_data.drop(columns=\"Net Income Flag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**\n",
    "\n",
    "- Look at the heatmap again. What do the dark blue and dark red colors mean?\n",
    "    - Look at some of the features that have dark blue and dark red colors. Can you understand why they have such colors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer: Your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we look at how the features are correlated with the target variable. We can do this by plotting the correlation of each feature with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the correlation between features and target. We use absolute values here.\n",
    "corr_to_bankrupt = bank_data.drop(columns=\"Bankrupt?\").corrwith(bank_data[\"Bankrupt?\"]).abs().sort_values()\n",
    "plt.figure(figsize=(20,4))\n",
    "sns.barplot(data=corr_to_bankrupt)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**\n",
    "- Are the results as you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer: Your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A second dataset\n",
    "\n",
    "For illustration purposes, we will also use another, much simpler, dataset first. This is also your chance to explore the dataset by yourself. The dataset used here is [credit card fraud dataset](https://www.kaggle.com/datasets/dhanushnarayananr/credit-card-fraud/data). \n",
    "\n",
    "\n",
    "**EXERCISE**\n",
    "\n",
    "1. Load the dataset. The dataset is named card_data.csv\n",
    "2. Print some basic information about the dataset and statistics about the different features. \n",
    "3. Visualize the class distribution pie plot, the histogram for all features, and any other useful plots you want to visualize.\n",
    "4. Comment on the label distribution and feature distribution. Is the dataset unbalanced? Do you see any anomalies in the features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Add new cells to do your experiments*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3yCr6_3hzl6"
   },
   "source": [
    "## Training a classifier\n",
    "\n",
    "Next, we will use the dataset to train a classifier. You might remember from the lecture that we need to split the dataset into a training and a test set. We will use the training set to train the classifier and the test set to evaluate its performance. We do not create an explicit validation set here, but use cross-validation to tune the hyperparameters of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the necessary functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeuDuXvcjKKh"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before splitting the dataset, we remove the target variable from the dataset and store it in a separate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlgotM9giytV"
   },
   "outputs": [],
   "source": [
    "# Define the prediction target\n",
    "target_name = \"Bankrupt?\"\n",
    "X = np.array(bank_data.drop(columns = target_name))\n",
    "y = np.array(bank_data[target_name])\n",
    "\n",
    "# Randomly shuffle the dataset, and use 20% of the data as test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size = 0.2, random_state = 2024, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also already set up the cross-validation strategy. We will use the variable `kf` later whenever we need to indicate how we want to split the data in the cross-validation strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use K-fold cross validation\n",
    "kf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Stratified\" means we divide the training data into K folds such that in each fold there are the same proportion of bankrupt cases as in the original dataset. Otherwise, there may be folds without positve (bankrupt) cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "- Do the same for the credit card fraud dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_name = ...\n",
    "X_card = ...\n",
    "y_card = ...\n",
    "X_train_card, X_test_card, y_train_card, y_test_card = ...\n",
    "kf_card = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdrYlsZ8itiJ"
   },
   "source": [
    "## Experiments with simple classifiers\n",
    "\n",
    "In our first experiments, we will use the simpler credit card fraud dataset. We will later show how the different classifiers perform on the bankruptcy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import the classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqs6MyEAm8ur"
   },
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAX2YUDlE9C6"
   },
   "source": [
    "We start with the simplest classifier, the k-nearest neighbors classifier. The k-nearest neighbors classifier assigns a sample to the majority class of its k nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuIhOJmtFcUF"
   },
   "source": [
    "We have already done dataset splitting. The next steps are:\n",
    "1. Train the classifier on the training set\n",
    "2. Calculate the score on the validation set\n",
    "3. Alternate the classifier hyperparameters\n",
    "4. Repeat 1-3 until you find the best hyperparameters\n",
    "5. Retrain the classifier using the best hyperparameters on the union of training and validation set.\n",
    "\n",
    "\n",
    "*Note: The following code is slightly complicated and may exceed what you have already learned about Python. Don't worry, you don't need to understand every detail. The important thing is to understand the general idea of the code, which you should grasp from the comments and by identifying the parts you are already familiar with.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "hWc6STiolOyj",
    "outputId": "f64d6896-d977-4a59-8660-730bcd0488d9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# We use KNN as an example to showcase the general machine learning pipeline\n",
    "# Step 1-3: Train -> validation -> change hyperparameters\n",
    "\n",
    "results = [] # A list, which stores the results\n",
    "\n",
    "for n in range(1, 5): # loop 5 times to try n_neighbors from n=1 to 5\n",
    "\n",
    "  knn = KNeighborsClassifier(n_neighbors = n) # train classifier with n neighbors \n",
    "  \n",
    "  scores = []  # A list that stores the results\n",
    "  \n",
    "  # loop over the 5 folds\n",
    "  for train_index, test_index in kf_card.split(X_train_card, y_train_card):\n",
    "    \n",
    "    # Get K-1 folds for training, 1 fold for testing\n",
    "    X_train_card_fold, X_test_card_fold = X_train_card[train_index], X_train_card[test_index]\n",
    "    y_train_card_fold, y_test_card_fold = y_train_card[train_index], y_train_card[test_index]\n",
    "    \n",
    "    # Train the model\n",
    "    knn.fit(X_train_card_fold, y_train_card_fold)\n",
    "    \n",
    "    # Validate\n",
    "    y_pred_knn_fold = knn.predict(X_test_card_fold)\n",
    "    scores.append(recall_score(y_test_card_fold, y_pred_knn_fold))\n",
    "  \n",
    "  # Get the average validation score\n",
    "  results.append(np.mean(scores))\n",
    "\n",
    "print(\"Best validation score(recall)\", max(results))\n",
    "print(\"Best n_neighbors\", results.index(max(results)) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, 5), results)\n",
    "plt.xlabel('n_neighbors')\n",
    "plt.ylabel('Cross-validation score')\n",
    "plt.title('Cross-validation score vs. n_neighbors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**\n",
    "\n",
    "- Why did we choose the recall as a metric? What would be other possible choices, suitable for this application?\n",
    "- Use the best parameter, and retrain a KNN classifier. \n",
    "- How is the performance of the KNN on the test set? Is it better than the performance on the validation set? Remember that we use recall as the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = ...  # Fit a KNN classifier\n",
    "\n",
    "y_pred_knn = ... # prediction on the test set\n",
    "\n",
    "# Calculate accuracy, precision, recall, f1-score. Print them out.\n",
    "print(\"Balanced accuracy\", balanced_accuracy_score(y_test_card, y_pred_knn))\n",
    "print(\"Accuracy\", accuracy_score(y_test_card, y_pred_knn))\n",
    "print(\"Precision\", precision_score(y_test_card, y_pred_knn))\n",
    "print(\"Recall\", recall_score(y_test_card, y_pred_knn))\n",
    "print(\"F1-score\", f1_score(y_test_card, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer: Space for your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKGMoeMlm_HC"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TLB-qqgG7nV"
   },
   "source": [
    "Let's try another simple classifier, logistic regression. Logistic regression is a linear classifier that uses the logistic function to predict the probability of a sample belonging to a class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDY3mnnbHLPZ"
   },
   "source": [
    "The `GridSearchCV()` function abstracts the pipeline we have used in the KNN case, including looping through all hyperparameters, finding the best ones and retraining the model on the whole training set. In the following, we will use this function instead of writing the parameter searching process explicitly.\n",
    "\n",
    "The parameter that we are optimizing is called `C` in scikit learn and is the inverse of the regularization parameter $\\lambda$ that was introduced for regression. It just penalizes large coefficients in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ud4hPABjmduM",
    "outputId": "2a919728-11bb-492c-c6ce-89c3a11a1a10"
   },
   "outputs": [],
   "source": [
    "# Train logistic classifier\n",
    "lr = LogisticRegression(max_iter=10000)\n",
    "params = {\n",
    "    \"C\": [0.01, 0.1, 1, 10],\n",
    "}\n",
    "grid_search_lr = GridSearchCV(lr, params, cv=kf_card, scoring=\"recall\", refit=True)\n",
    "grid_search_lr.fit(X_train_card, y_train_card)\n",
    "print(\"Best parameters:\", grid_search_lr.best_params_)\n",
    "\n",
    "# Validation score\n",
    "print(\"Validation score(recall):\", grid_search_lr.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We validate the model on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bKXJv82Gr4kc",
    "outputId": "547b827c-0ae5-4ad7-d01e-8bb48fddeab0"
   },
   "outputs": [],
   "source": [
    "# Predict with logistic classifier. Note that we can directly use the best model from grid search\n",
    "y_pred_lr = grid_search_lr.predict(X_test_card)\n",
    "\n",
    "# Calculate accuracy, precision, recall, f1-score\n",
    "print(\"Balanced accuracy\", balanced_accuracy_score(y_test_card, y_pred_lr))\n",
    "print(\"Accuracy\", accuracy_score(y_test_card, y_pred_lr))\n",
    "print(\"Precision\", precision_score(y_test_card, y_pred_lr))\n",
    "print(\"Recall\", recall_score(y_test_card, y_pred_lr))\n",
    "print(\"F1-score\", f1_score(y_test_card, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As logistic regression is a probabilistic classifier, we can also plot the ROC curve and the precision recall curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 916
    },
    "id": "sEWQAJLcfr_D",
    "outputId": "ff41ba11-63e4-41c6-9cd5-53c8cb65eac2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay, RocCurveDisplay\n",
    "\n",
    "y_score_lr_card = grid_search_lr.predict_proba(X_test_card)[:, 1]\n",
    "\n",
    "RocCurveDisplay.from_predictions(y_test_card, y_score_lr_card)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrecisionRecallDisplay.from_predictions(y_test_card, y_score_lr_card)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**\n",
    "\n",
    "- How do you interpret the ROC curve and the precision recall curve? Are the results good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer: Space for you answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate which parameters are the most important for the logistic regression model. We can do this by looking at the coefficients of the model. Note, however, that you need to scale the features before interpreting the coefficients.\n",
    "\n",
    "**EXERCISE**\n",
    "\n",
    "- Scale the features using the `StandardScaler` and retrain the logistic regression model.\n",
    "  - *Hint: * Check out the lecture notebook `classification.ipynb` to see how the scaler is used.\n",
    "- Print the coefficients of the model. Which features are the most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = ...\n",
    "\n",
    "X_train_scaled = ...\n",
    "X_test_scaled = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer: Space for your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Plfm0PJWnBUu"
   },
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4rV47t3J0Xq"
   },
   "source": [
    "Next we train a decision tree. The most important hyperparameter for a decision tree would be its max depth. Usually the deeper the tree is, the more powerful it is. But deep trees might encounter severe overfitting problems. We will see that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K3cbUTHcm8Rw",
    "outputId": "913f2048-a097-43e8-d154-ab4bf0da8859"
   },
   "outputs": [],
   "source": [
    "# [Read]\n",
    "# Train desicion tree\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "params = {\n",
    "    \"max_depth\": [None, 1, 2, 3, 4, 5],\n",
    "}\n",
    "\n",
    "grid_search_dt = GridSearchCV(decision_tree, params, cv=kf_card, scoring=\"recall\", refit=True)\n",
    "grid_search_dt.fit(X_train_card, y_train_card)\n",
    "print(\"Best parameters:\", grid_search_dt.best_params_)\n",
    "\n",
    "# Validation score\n",
    "print(\"Validation score(recall):\", grid_search_dt.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4pbpuzbjlH5N",
    "outputId": "86787555-f052-41db-d8d3-6bef25d68963"
   },
   "outputs": [],
   "source": [
    "# Predict with decision tree\n",
    "y_pred_dt = grid_search_dt.predict(X_test_card)\n",
    "\n",
    "# Calculate accuracy, precision, recall, f1-score\n",
    "print(\"Balanced accuracy\", balanced_accuracy_score(y_test_card, y_pred_dt))\n",
    "print(\"Accuracy\", accuracy_score(y_test_card, y_pred_dt))\n",
    "print(\"Precision\", precision_score(y_test_card, y_pred_dt))\n",
    "print(\"Recall\", recall_score(y_test_card, y_pred_dt))\n",
    "print(\"F1-score\", f1_score(y_test_card, y_pred_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJjUeVa0KQ7S"
   },
   "source": [
    "The decision tree almost perfectly classifies every samples. To get a more intuitive understanding of the decision tree, we can visualize a tree with small depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mciX-weXR5kj",
    "outputId": "976d48d3-c228-4c53-9236-723c1056a7bf"
   },
   "outputs": [],
   "source": [
    "# [Read]\n",
    "# Visualize a decision tree that is shallower\n",
    "# Click on the image to zoom in, or you can download it to have a more clear view.\n",
    "decision_tree = DecisionTreeClassifier(max_depth=2)\n",
    "decision_tree.fit(X_train_card, y_train_card)\n",
    "plt.figure(figsize=(50,50))\n",
    "target_names = [\"not fraud\", \"fraud\"]\n",
    "feature_names = [col for col in card_data.columns if col not in target_names]\n",
    "plot_tree(decision_tree, class_names=target_names, feature_names=feature_names, filled=True, impurity=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**\n",
    "\n",
    "- Visualize a decision tree with depth 3. What can you learn from the tree?\n",
    "- Can you compare it with the logistic regression model? Which one is more interpretable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer: Your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5upRGDglfif"
   },
   "source": [
    "The above examples show that logistic regression, KNN and decision trees perform already pretty good. This is because the dataset is fairly easy. Let's see how they perform on the more difficult dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification on the bankruptcy dataset\n",
    "\n",
    "Now that we have found a good model to classify credit card fraud, let's switch to the bankruptcy dataset, which is a bit more challenging.\n",
    "\n",
    "**EXERCISE**\n",
    "\n",
    "- Train a KNN, a logistic regression and a decision tree classifier on the bankruptcy dataset using the same pipeline as above. Report the accuracy, balanced accuracy, precision, recall and f1 score on the training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "S-dlibp36D46",
    "outputId": "2579ece7-d3cb-4258-ca4f-0c6a0cdcd242"
   },
   "outputs": [],
   "source": [
    "# Train classifiers\n",
    "lr_bankruptcy = ...\n",
    "lr_params_bankruptcy = ...\n",
    "\n",
    "grid_search_lr_bankruptcy = ...\n",
    "\n",
    "knn_bankruptcy = ...\n",
    "knn_params_bankruptcy = ...\n",
    "grid_search_knn_bankruptcy = ...\n",
    "\n",
    "decision_tree_bankruptcy = ...\n",
    "dt_params_bankruptcy = ...\n",
    "\n",
    "grid_search_dt_bankruptcy = ...\n",
    "\n",
    "\n",
    "# Evaluate the classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**\n",
    "- Which methods do overfit? Why do you think so?\n",
    "- Why do you think accuracy is so high, even though recall and precision are low. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer: Space for your answers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeOf4QK0K4rE"
   },
   "source": [
    "### Imbalanced dataset\n",
    "\n",
    "Remember that the dataset is quite imbalanced, as bankruptcy cases are rare. To solve this problem, we can give different weights to the bankruptcy cases and non-bankruptcy cases so that the classifier can pay more attention to the rare cases. The following code shows how to do this in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "Su4i01QJ6XSp",
    "outputId": "e5d04c11-28fd-48e9-a920-85e27db795a9"
   },
   "outputs": [],
   "source": [
    "# [Read]\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# Compute the weights of the samples based on the balance in the dataset\n",
    "weight = compute_sample_weight(class_weight=\"balanced\", y=y_train)\n",
    "\n",
    "# Train classifiers\n",
    "dt = DecisionTreeClassifier()\n",
    "dt_params = {\n",
    "    \"max_depth\": [None, 1, 2, 3, 4, 5],\n",
    "}\n",
    "grid_search_dt = GridSearchCV(dt, dt_params, cv=kf, scoring=\"recall\", refit=True)\n",
    "\n",
    "# reweight during training\n",
    "grid_search_dt.fit(X_train, y_train, sample_weight=weight) \n",
    "\n",
    "# predict on the test set and calculate the precision, recall, f1-score\n",
    "y_pred_dt = grid_search_dt.predict(X_test)\n",
    "\n",
    "print(\"Balanced accuracy\", balanced_accuracy_score(y_test, y_pred_dt))\n",
    "print(\"Accuracy\", accuracy_score(y_test, y_pred_dt))\n",
    "print(\"Precision\", precision_score(y_test, y_pred_dt))\n",
    "print(\"Recall\", recall_score(y_test, y_pred_dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**\n",
    "\n",
    "- How do the metrics change? Can you explain this?\n",
    "- Does the overfitting problem improve? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer: Space for your answers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evRWOyZwnOJa"
   },
   "source": [
    "## Experiment with ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHIQ5pD9Jr8X"
   },
   "source": [
    "In this section we will explore two ensemble methods: random forest and XGBoost. They have more capabilities than the three simple classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pgPa3VCPmdj"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpz_QqF6qWLY"
   },
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5C_bOs4VqBQ5"
   },
   "source": [
    "Random Forest uses an ensemble technique known as bagging **(Bootstrap Aggregating)** which helps to improve stability and accuracy. To produce a prediction that is more reliable and accurate, it constructs several decision trees and combines them. Every tree in the ensemble is constructed using a bootstrap sample, which is a sample taken from the training set with replacement. Furthermore, Random Forest only takes into account a random subset of features for splitting at each node while constructing separate trees, increasing tree diversity and producing a more resilient model that is less prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code illustrates how to train a random forest classifier. \n",
    "The parameter `n_estimators` is the number of trees in the forest. The parameter `max_depth` is the maximum depth of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XnBdMB2aprKS",
    "outputId": "1c7c7f38-9f3b-47ba-df7e-6cf2e905a509"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train random forest\n",
    "weight = compute_sample_weight(class_weight=\"balanced\", y=y_train)\n",
    "rf = RandomForestClassifier(bootstrap=True) # Bootstrapping specified here\n",
    "rf_params = {\n",
    "    \"n_estimators\": [10, 50, 100],\n",
    "    \"max_depth\": [None, 1, 2, 3, 4, 5],\n",
    "}\n",
    "grid_search_rf = GridSearchCV(rf, rf_params, cv=kf, scoring=\"recall\", refit=True)\n",
    "grid_search_rf.fit(X_train, y_train, sample_weight=weight)\n",
    "print(\"Best parameters:\", grid_search_rf.best_params_)\n",
    "\n",
    "# Validation score  \n",
    "print(\"Validation score(recall):\", grid_search_rf.best_score_)\n",
    "y_pred_rf = grid_search_rf.predict(X_test)\n",
    "y_pred_proba_rf = grid_search_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate accuracy, precision, recall, f1-score\n",
    "print(\"Balanced accuracy\", balanced_accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Accuracy\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Precision\", precision_score(y_test, y_pred_rf))\n",
    "print(\"Recall\", recall_score(y_test, y_pred_rf))\n",
    "print(\"F1-score\", f1_score(y_test, y_pred_rf))\n",
    "\n",
    "# add prediction to results dictionary\n",
    "y_preds[\"random_forest\"] = y_pred_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_predictions(y_test, y_pred_proba_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrecisionRecallDisplay.from_predictions(y_test, y_pred_proba_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97HLF-rnqdHs"
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eQPN8xLqB7r"
   },
   "source": [
    "XGBoost develops one tree at a time, correcting faults caused by previously trained trees, in contrast to Random Forest, where each tree is generated independently and the results are aggregated at the end. Trees are planted until none remain. The model uses a gradient descent algorithm to minimize the loss when adding new models. This sequential addition of weak learners (trees) ensures that the shortcomings of previous trees are corrected. The additive model known as gradient boosting is implemented by XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "I6e7Y0-tqQ9w",
    "outputId": "34544eb6-8bf2-48f5-f476-f5267c0a3d94"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "weight = compute_sample_weight(class_weight=\"balanced\", y=y_train)\n",
    "xgb = XGBClassifier()\n",
    "xgb_params = {\n",
    "    \"n_estimators\": [10, 50, 100],\n",
    "    \"max_depth\": [None, 1, 2, 3, 4, 5],\n",
    "}\n",
    "grid_search_xgb = GridSearchCV(xgb, xgb_params, cv=kf, scoring=\"recall\", refit=True)\n",
    "grid_search_xgb.fit(X_train, y_train, sample_weight=weight)\n",
    "\n",
    "print(\"Validation score(recall): \", grid_search_xgb.best_score_)\n",
    "\n",
    "# 2. Predict with random forest\n",
    "y_pred_xgb = grid_search_xgb.predict(X_test)\n",
    "y_score_xgb = grid_search_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 3. Calculate accuracy, precision, recall, f1-score\n",
    "print(\"Balanced accuracy\", balanced_accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"Accuracy\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"Precision\", precision_score(y_test, y_pred_xgb))\n",
    "print(\"Recall\", recall_score(y_test, y_pred_xgb))\n",
    "print(\"F1-score\", f1_score(y_test, y_pred_xgb))\n",
    "\n",
    "# add prediction to results dictionary\n",
    "y_preds[\"xgboost\"] = y_pred_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_predictions(y_test, y_score_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrecisionRecallDisplay.from_predictions(y_test, y_score_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code gives an overview of all the results of the different classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"logistic_regression\": [],\n",
    "    \"knn\": [],\n",
    "    \"decision_tree\": [],\n",
    "    \"random_forest\": [],\n",
    "    \"xgboost\": []\n",
    "}\n",
    "\n",
    "for classifier in results.keys():\n",
    "  results[classifier].append(accuracy_score(y_test, y_preds[classifier]))\n",
    "  results[classifier].append(balanced_accuracy_score(y_test, y_preds[classifier]))\n",
    "  results[classifier].append(precision_score(y_test, y_preds[classifier]))\n",
    "  results[classifier].append(recall_score(y_test, y_preds[classifier]))\n",
    "  results[classifier].append(f1_score(y_test, y_preds[classifier]))\n",
    "\n",
    "\n",
    "results = pd.DataFrame.from_dict(results, orient='index', columns=[\"accuracy\", \"balanced_accuracy\", \"precision\", \"recall\", \"f1-score\", \"roc-auc\", \"pr-auc\"])\n",
    "print(\"---------------Test results for all methods---------------\")\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPBq8o8wqx85"
   },
   "source": [
    "**EXERCISE**\n",
    "\n",
    "- Look at the final results. Which one is the best? Why do you think so?\n",
    "- If the goal is to identify as many bankruptcy cases as possible, which classifier should you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9QGZJLZp0yK"
   },
   "source": [
    "*Answer: Space for your answers*\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
