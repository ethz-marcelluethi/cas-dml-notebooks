{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation\n",
    "\n",
    "Automatic differentiation (autodiff) is a powerful tool used in deep learning and other optimization based learning algorithms to compute gradients efficiently. \n",
    "\n",
    "In the first part of this notebook, we will introduce the mechanics of autodiff in Tensorflow. In the second part, we will use autdiff to implement a simple linear regression. While simple, this example is representative of the kind of optimization problems that are solved in deep learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the necessary libraries. In addition to Tensorflow, we will use numpy for some basic numeric operations and matplotlib for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of automatic differentiation is to compute the gradient of a function. In the context of machine learning, the function is typically the loss function.\n",
    "We start, however, with a much simpler function, namely the function $f(x) = x^2$. From calculus, we know that the derivative of this function is $f'(x) = 2x$. We will use autodiff to compute this derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compute with normal values in Python, as in the computation \n",
    "```python\n",
    "x = 3\n",
    "y = x**2\n",
    "```\n",
    "the value of `y` is computed and stored in memory. Python does not keep track of the operations that were used to compute `y`. Consequently, we cannot compute the derivative of `y` with respect to `x` using Python. \n",
    "\n",
    "To make automatic differentiation possible, we need to use a special data structure that keeps track of the operations used to compute a value. In Tensorflow, this data structure is called a `tf.Variable`. It tells Tensorflow to keep track of the operations used to compute the value of the variable. To instruct Tensorflow to compute the gradient of a variable, we need to perform the operations within a `tf.GradientTape` context, as illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(7.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now possible to compute the gradient of `y` with respect to `x`. The result is the expected value of `2*x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = tape.gradient(y, x)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "\n",
    "- Change the value of `x` to a different value and re-run the cell. Is the computation correct?\n",
    "- Experiment with different functions, such as `y = x**3` or `y = x**2 + 3*x`. Can you compute the gradient of these functions using autodiff? \n",
    "- What happens if you try to compute the gradient of a function that is not a function of a `tf.Variable`, i.e. if you change `x` to be a normal Python variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation of gradients is not restricted to tracking only one variable. We can compute the gradient of a function with respect to multiple variables. In the example below, we compute the gradient of the function $f(x, y) = x^2 + y$ with respect to both $x$ and $y$. The expected result is $2x$ if we differentiate with respect to $x$ and $1$ if we differentiate with respect to $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(1, dtype=tf.float32)\n",
    "y = tf.Variable(2, dtype=tf.float32)\n",
    "\n",
    "# We use persistent=True so that we can call gradient twice on the same tape\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = x * x + y       \n",
    "\n",
    "dz_dx = tape.gradient(z, x)\n",
    "dz_dy = tape.gradient(z, y)\n",
    "\n",
    "print(dz_dx)\n",
    "print(dz_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most machine learning models we have many parameters that we need to optimize. We would not explicitly create a `tf.Variable` for each parameter. Instead, we would typically use a `tf.Variable` to store all parameters at once. We call such a variable a vector (or 1d tensor). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable([1, 2], dtype=tf.float32)\n",
    "\n",
    "# We use persistent=True so that we can call gradient twice on the same tape\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = x[0] * x[0] + 5 * x[1]       \n",
    "\n",
    "dz_dx = tape.gradient(z, x)\n",
    "\n",
    "print(dz_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the result has the same values, but we get it with one call to `tape.gradient` instead of two. Furthermore, the result is returned as a vector, with one entry for each variable. This vector is called the gradient vector of $z$ and often written as $\\nabla z$ or $\\text{grad} z$.\n",
    "\n",
    "Working with vectors is more efficient and more convenient than working with individual variables, especially when we have many parameter, as is the case in deep learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how to compute gradients with autodiff, we will use this tool to implement a simple linear regression. \n",
    "\n",
    "We start by generating some synthetic data. We will generate a dataset with 100 samples. Each sample consists of a single feature $x$ and a target value $y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_true = 2 # true y-intercept\n",
    "w_true = 3 # true slope\n",
    "\n",
    "x = np.random.rand(100).astype(np.float32)\n",
    "noise = np.random.normal(scale=0.1, size=len(x))\n",
    "y = w_true * x + b_true + noise\n",
    "\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the regression, we define the variables, the model and the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_fn(y, y_pred):\n",
    "    # note that y and y_pred are vectors (1d tensors) and hence also y - y_pred\n",
    "    # reduce_mean computes the mean value of all elements in \n",
    "    return tf.reduce_mean(tf.square(y - y_pred))\n",
    "\n",
    "def model(w, b, x):\n",
    "    return w * x + b   # x is a vector of inputs. The operation leads to a vector of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some predictions with our model. We plot the predictions against the true values and also compute the loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the weights and bias\n",
    "w = tf.Variable(0.0)\n",
    "b = tf.Variable(0.0)\n",
    "\n",
    "predictions = model(w, b, x)\n",
    "plt.scatter(x, predictions, label='Predictions', c='r')\n",
    "plt.scatter(x, y, label='True', c='b')\n",
    "plt.legend()\n",
    "loss = loss_fn(y, predictions)\n",
    "\n",
    "print(\"the loss before training is\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to optimize the model. We will compute gradients and do a gradient descent step to update the weights of the model.  We will iterate over the dataset 100 times and update the weights of the model after each iteration. In each iteration, we will compute the loss and print it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(0.0)\n",
    "b = tf.Variable(0.0)\n",
    "\n",
    "lr = 0.1  # learning rate\n",
    "number_of_iterations = 100\n",
    "\n",
    "history = []\n",
    "for iter in range(number_of_iterations):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(w, b, x)\n",
    "        loss = loss_fn(y, y_pred)\n",
    "    \n",
    "    dw, db = tape.gradient(loss, [w, b])\n",
    "\n",
    "    # to update the weights, we cannot use w = w - lr * dw but \n",
    "    # we have to use w.assign(w - lr * dw) in order to keep \n",
    "    # the operation recorded by the tape\n",
    "    w.assign(w - lr * dw) # update the weights\n",
    "    b.assign(b - lr * db) # update the bias\n",
    "    \n",
    "    history.append(loss.numpy())\n",
    "\n",
    "    if iter % 10 == 0: # we print the loss every 10 iterations\n",
    "        print(f'Iteration {iter}: Loss: {loss.numpy()}')\n",
    "print(f'Final weights: {w.numpy()} and bias: {b.numpy()}')\n",
    "\n",
    "plt.plot(history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the predictions again and plot them. We should see that the predictions are much closer to the true values now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(w, b, x)\n",
    "plt.scatter(x, predictions, label='Predictions', c='r')\n",
    "plt.scatter(x, y, label='True', c='b')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "- What do you observe towards the end of the training? Does the loss decrease in each iteration?\n",
    "- Experiment with different learning rates. What happens if you set the learning rate to a very high value, such as 1.0? What happens if you set it to a very low value, such as 0.0001?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression (Exercise)\n",
    "\n",
    "In this section you will adapt the code to implement logistic regression. Remember, the difference to linear regression is on one hand the model, which passes the logits (linear predictions) through a sigmoid function, and on the other hand the loss function, which is the binary cross entropy loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code generates synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate classification data for a simple logistic regression model\n",
    "\n",
    "n = 100\n",
    "x = np.random.rand(n, ) - 0.5\n",
    "b_true = 0.0\n",
    "w_true = 3.0\n",
    "noise_var = 0.1\n",
    "y = np.round(1 / (1 + np.exp(- (b_true + x * w_true ))) + np.random.normal(0, noise_var, size=n))\n",
    "\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the sigmoid function from `tf.nn.sigmoid` `tf.math.log` for computing the logarithm. With these functions you should be able to implement the loss function and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_lr(y, y_pred):\n",
    "    return None # Replace None with the correct loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_lr(w, b, x):\n",
    "    return None # Replace None with the correct loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some predictions with the true weights to make sure the model is working\n",
    "# Also test the loss function\n",
    "\n",
    "w = ...\n",
    "b = ...\n",
    "predictions = ...\n",
    "loss = ...\n",
    "print(\"the loss before training is\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop remains unchanged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the weights and bias\n",
    "w = tf.Variable(0.0)\n",
    "b = tf.Variable(0.0)\n",
    "\n",
    "# define the learning rate\n",
    "lr = 0.3\n",
    "number_of_iterations = 1000\n",
    "\n",
    "history = []\n",
    "for iter in range(number_of_iterations):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred =model_lr(w, b, x)\n",
    "        loss = loss_fn_lr(y, y_pred)\n",
    "    dw, db = tape.gradient(loss, [w, b])\n",
    "    w.assign_sub(lr * dw)\n",
    "    b.assign_sub(lr * db)\n",
    "\n",
    "    history.append(loss.numpy())\n",
    "    if iter % 100 == 0:\n",
    "        print(f'Iteration {iter}: Loss: {loss.numpy()}')\n",
    "print(f'Final weights: {w.numpy()} and bias: {b.numpy()}')\n",
    "\n",
    "plt.plot(history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make again some predictions and plot them. You should see that the model is able to separate the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ... \n",
    "plt.scatter(x, predictions, label='Predictions', c='r')\n",
    "plt.scatter(x, y, label='True', c='b')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
