{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07c7314-a036-49f2-8dce-62b1137a5b9c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Regression\n",
    "After a first look at simple linear regression, we want to look at more complex details, potential issues, and how to resolve them. In order to focus on certain data science topics, we will use syntethic data for this part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd96e999-5391-479c-8c98-672593838f7f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3952ee1-dc53-44a7-b8e2-3f406e20c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cdca84-f029-402a-ab79-add77690a3ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn as sklearn\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, lasso_path\n",
    "from sklearn.metrics import r2_score, mean_squared_error, root_mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2a1af-9ca3-4f63-bc96-061e99c4fdb0",
   "metadata": {},
   "source": [
    "## Generate Data\n",
    "We define the following function to generate a data set (with `n_samples` many entries). As in the previous notebooks, this function returns a DataFrame, here with the attributes `x` and `y`. We will consider `x` as the independent variable and `y` as the dependent variable throughout the notebook.\n",
    "\n",
    "The function uses a random generator to generate random values ​​for the `x` values ​​and to introduce noise (e.g. a measurement error) on the `y` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb056b-258c-4a3a-bebc-a611dd39f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_df(n_samples = 20):\n",
    "    x = - 4 * np.random.uniform(-1, 1, n_samples)\n",
    "    y = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, n_samples)\n",
    "    \n",
    "    dataset_df = pd.DataFrame({ 'x': x, 'y': y})\n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae4e483-4b7c-4de6-8038-03b66f3fb4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_dataset_df(n_samples = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d24bba-076b-496b-bb57-fc3357c743fa",
   "metadata": {},
   "source": [
    "We now use this function to generate our dataset, which we will split into a training and test data set. We could of course also generate two separate datasets for training and testing separately, but we'll follow this appraoch to see how to split a single given dataset into a training and test set.\n",
    "\n",
    "As mentioned, the function uses a random generator. This random generator can be initialized with a specific value - the so-called seed. **You should get always set a seed so that the results are reproducible**, i.e. you can run the notebook again later and get the same results. The actual value, however, does not really matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1635de6d-698c-4a13-9033-10ddca9f4654",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba69935-6118-456f-b4b9-7545864ed743",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = generate_dataset_df(n_samples=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0dd5c4-648f-4c10-b32e-0cde8168a4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random sample of the data for training\n",
    "data_train = all_data.sample(frac=0.5, random_state=1)\n",
    "# gets the left out portion of the dataset\n",
    "data_test=all_data.drop(data_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27f3c5c-c5c6-450d-87d3-c871b963d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ffd8c-3656-4713-83ed-342aec915aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "plt.scatter(data_train['x'], data_train['y'], s=10)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3a7943-0d5f-4235-8016-b851f592a005",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "Again, we define a Regression model. One way to do so is the class `LinearRegression()` from `sklearn.linear_model`, which we have imported above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9fe19c-0158-48b0-95f3-a8b98a999388",
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_deg1 = LinearRegression()\n",
    "linreg_deg1.fit(data_train[['x']], data_train['y'])\n",
    "\n",
    "print('Coefficient: ')\n",
    "print(linreg_deg1.coef_)\n",
    "print('Intercept:')\n",
    "print(linreg_deg1.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b773c2d8-851b-4cc2-9074-eb2c8bdaee90",
   "metadata": {},
   "source": [
    "Let's plot the training data along with the fitted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e995f-f042-4df5-9aeb-5249a500b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_forPlot = pd.DataFrame(np.linspace(-4, 4, num=41), columns = ['x'])\n",
    "data_forPlot['y_lm_grad01'] = linreg_deg1.predict(data_forPlot[['x']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bb96e7-fb10-4205-93a0-06dd16548214",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "plt.scatter(data_train['x'], data_train['y'], \n",
    "            label='Training Data', color='b', s=10)\n",
    "plt.plot(data_forPlot['x'], data_forPlot['y_lm_grad01'],\n",
    "         label = 'Linear Modell',\n",
    "         linestyle=\"--\", color='r')\n",
    "\n",
    "plt.legend()\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2327897f-32a8-4c16-8a32-47f150d5278c",
   "metadata": {},
   "source": [
    "### Calculate & Visualize Residuals\n",
    "We define a function to get the prediction from a fitted model and compute the residuum values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5122727c-8632-4994-92b9-4994ae65af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_and_residuum(fitted_model, data_set, y_true):\n",
    "    y_pred = fitted_model.predict(data_set)\n",
    "    df_prediction = data_set.copy()\n",
    "    df_prediction['Prediction'] = y_pred\n",
    "    df_prediction['Residual'] = y_true - df_prediction['Prediction']\n",
    "    print('r2-Score: ' + str(r2_score(y_true, y_pred)))\n",
    "    print('MSE: ' + str(mean_squared_error(y_true, y_pred)))\n",
    "    print('RMSE: ' + str(root_mean_squared_error(y_true, y_pred)))\n",
    "    return df_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c8a85-be53-4523-90eb-f75a49777683",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_grad01_pred_res_train = get_prediction_and_residuum(linreg_deg1,\n",
    "                                                       data_train[['x']],\n",
    "                                                       data_train['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ea053-089d-4ee7-9fa0-0bf96e875ac1",
   "metadata": {},
   "source": [
    "We make a plot comparing the predictor values (on the x axis) with the residuum values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532507f9-d1ba-4b16-854c-4beff024e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_grad01_pred_res_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad5c62b-03e6-4e1b-8012-7df3fd30dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "plt.scatter(data_train['x'], lm_grad01_pred_res_train['Residual'],\n",
    "            label='Training Data', s=10)\n",
    "plt.legend()\n",
    "\n",
    "plt.legend()\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Residual ($y-\\hat{y}$) of Linear Regression')\n",
    "\n",
    "plt.plot(plt.xlim(), [0, 0], linestyle=\"--\", color='k')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d78c514-3dc4-41e7-9b09-1692eb13d3ce",
   "metadata": {},
   "source": [
    "We can see a clear structure in these values - for lower values of x, the residuum is alwys negative, in the medium range it is always positive, and for higher values the residuum is negative again. To capture this nonlinear behavior, we define a more complex, non-linear model (in the original values) by defining a regression including the quadratic values of `x`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d29a11a-4f15-4af7-b54b-0d9c431d9ba6",
   "metadata": {},
   "source": [
    "## Quadratic Regression\n",
    "We first do the feature transformation by defining a further feature `x2` as the square of `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0521416-954a-4655-87dd-01fe06de8597",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['x2'] = data_train['x']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd154a-b5e8-4c52-bec6-4a908b6f7e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_deg2_1 = LinearRegression()\n",
    "linreg_deg2_1.fit(data_train[['x', 'x2']], data_train['y'])\n",
    "\n",
    "print('Coefficient: ')\n",
    "print(linreg_deg2_1.coef_)\n",
    "print('Intercept:')\n",
    "print(linreg_deg2_1.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e54eb-fd7d-4ea8-a8e9-d1d2291f659e",
   "metadata": {},
   "source": [
    "### Data transformation\n",
    "The `PolynomialFeatures` from `scikit-learn` uses arrays for the independent and dependent variables - and not DataFrames, as with `statmodels`. We therefore first store the independent variable `x` and the dependent variable `y` as an array, both for the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea2c33f-d806-453e-85ac-a125f6c81a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = data_train[['x']]\n",
    "train_y = data_train[['y']]\n",
    "\n",
    "test_x = data_test[['x']]\n",
    "test_y = data_test[['y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e521ff0-28b6-4f84-a1d1-35e9436e9fdb",
   "metadata": {},
   "source": [
    "Now we can produce the polynomial features. We also convert them into a DataFrame, so that we have the same structure as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7253c2d-3db7-4926-9885-23fd54f25972",
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features02 = PolynomialFeatures(degree=2)\n",
    "train_x_poly02 = polynomial_features02.fit_transform(train_x)\n",
    "train_x_poly02_df = pd.DataFrame(train_x_poly02, columns=['x0', 'x1', 'x2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fde4f84-0a12-4fa5-ad5b-0556a26a3e20",
   "metadata": {},
   "source": [
    "We define a new model `model02` for the quadratic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e6e81-ec51-4c40-97ec-cd663b0ac7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model02 = LinearRegression()\n",
    "model02.fit(train_x_poly02_df, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96871ec-a747-489b-95af-d7f1c46b1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficient: ')\n",
    "print(model02.coef_)\n",
    "print('Intercept:')\n",
    "print(model02.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52682d1d-2f30-4ecf-9302-ce07499629b6",
   "metadata": {},
   "source": [
    "As we will be using this functionality over and over again, we pack it into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed48c77-3ecd-4b97-81ad-79763f199a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_coeff_and_intercept(model):\n",
    "    print(\"Model Coefficients: \")\n",
    "    print(model.coef_)\n",
    "    print(\"Intercept: \")\n",
    "    print(model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e912021-7e1d-4a1e-8210-1405765320c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_coeff_and_intercept(model02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c731b2ff-c276-40e2-b6d4-3854665d5b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_grad02_pred_res_train = get_prediction_and_residuum(model02,\n",
    "                                                       train_x_poly02_df,\n",
    "                                                       data_train['y'].reset_index()['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7da5a5-0e14-419b-be68-79ef4cf5c965",
   "metadata": {},
   "source": [
    "Again, we prepare a lot of the training data and the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5534b461-2e74-4d31-b11b-f9d8f73dea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_forPlot_2 = polynomial_features02.fit_transform(data_forPlot[['x']])\n",
    "data_forPlot_2_df = pd.DataFrame(data_forPlot_2, columns=['x0', 'x1', 'x2'])\n",
    "data_forPlot['y_lm_grad02'] = model02.predict(data_forPlot_2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475283f9-af40-40c4-bf53-f40189feda9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "plt.scatter(data_train['x'], data_train['y'], label='Training Data', color='b', s=10)\n",
    "plt.plot(data_forPlot['x'], data_forPlot['y_lm_grad01'], label = 'Linear Model', linestyle=\"--\", color='r')\n",
    "plt.plot(data_forPlot['x'], data_forPlot['y_lm_grad02'], label = 'Quadratic Model', linestyle=\"--\", color='m')\n",
    "\n",
    "plt.legend()\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6631e-9929-4574-a0f7-e44aaa9b30e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "plt.scatter(data_train['x'], lm_grad02_pred_res_train['Residual'], label='Training Data', s=10)\n",
    "plt.legend()\n",
    "\n",
    "plt.legend()\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Residual ($y-\\hat{y}$) of Quadratic Regression')\n",
    "\n",
    "plt.plot(plt.xlim(), [0, 0], linestyle=\"--\", color='k')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90784406-bd7e-42c6-9ea0-011c79a06d80",
   "metadata": {},
   "source": [
    "## Qubic Regression\n",
    "As before, we do the feature transformation by defining a further feature and then fit a linear regression with OLS (ordinary least squares):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfca47a3-c00c-43bc-93e7-b5486622ba56",
   "metadata": {},
   "source": [
    "**EXERCISE:** Analoguous to the quadratic regression above, do a regression with a polynomial of degree 3. Train and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d91bdb-274c-4a0d-ad59-4ebe751b78b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial_features03 = ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03e8d40-066f-4fd5-8fd6-a1ab3e14cac6",
   "metadata": {},
   "source": [
    "Again we plot the models and the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f261a370-11e8-4917-98ba-d2a3e7ef5ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_forPlot_3 = polynomial_features03.fit_transform(data_forPlot[['x']])\n",
    "data_forPlot_3_df = pd.DataFrame(data_forPlot_3, columns=['x0', 'x1', 'x2', 'x3'])\n",
    "data_forPlot['y_lm_grad03'] = model03.predict(data_forPlot_3_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd551bf-99c5-4297-bbfa-e51136d59e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "plt.scatter(data_train['x'], data_train['y'], \n",
    "            label='Training Data', color='k', s=10)\n",
    "plt.plot(data_forPlot['x'], data_forPlot['y_lm_grad01'], label = 'Linear Model', linestyle=\"--\", color='r')\n",
    "plt.plot(data_forPlot['x'], data_forPlot['y_lm_grad02'], label = 'Quadratic Model', linestyle=\"--\", color='m')\n",
    "plt.plot(data_forPlot['x'], data_forPlot['y_lm_grad03'], label = 'Qubic Model', linestyle=\"--\", color='b')\n",
    "\n",
    "plt.legend()\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49668fbc-9060-4ce9-8b19-fa1c199d79a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "plt.scatter(data_train['x'], lm_grad03_pred_res_train['Residual'], label='Training Data', s=10)\n",
    "plt.legend()\n",
    "\n",
    "plt.legend()\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Residual ($y-\\hat{y}$) of Qubic Regression')\n",
    "\n",
    "plt.plot(plt.xlim(), [0, 0], linestyle=\"--\", color='k')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1decf-4b81-4e16-b1ae-53a06bd54a7e",
   "metadata": {},
   "source": [
    "## Polynomial Regression, Degree 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05a9e7c-8d3a-4957-89d1-dc3f458b4a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features04 = PolynomialFeatures(degree=4)\n",
    "train_x_poly04 = polynomial_features04.fit_transform(train_x)\n",
    "train_x_poly04_df = pd.DataFrame(train_x_poly04, columns=['x0', 'x1', 'x2', 'x3', 'x4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25968d3b-04aa-43d0-b4ae-4bf286ea04ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model04 = LinearRegression()\n",
    "model04.fit(train_x_poly04_df, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26dcca7-7464-40d0-82f8-99c64dc759e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_coeff_and_intercept(model04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec524e-b421-4d71-80e7-3a679af5d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_forPlot_4 = polynomial_features04.fit_transform(data_forPlot[['x']])\n",
    "data_forPlot_4_df = pd.DataFrame(data_forPlot_4, columns=['x0', 'x1', 'x2', 'x3', 'x4'])\n",
    "data_forPlot['y_lm_grad04'] = model04.predict(data_forPlot_4_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4db865f-bf8a-4c40-add0-a48ce5454206",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_grad04_pred_res_train = get_prediction_and_residuum(model04,\n",
    "                                                       train_x_poly04_df,\n",
    "                                                       data_train['y'].reset_index()['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a987031c-dc1a-4c45-901e-ff352d067149",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "plt.scatter(data_train['x'], data_train['y'], \n",
    "            label='Training Data', color='k', s=10)\n",
    "plt.plot(data_forPlot['x'], data_forPlot['y_lm_grad01'], label = 'Linear Model', linestyle=\"--\", color='r')\n",
    "plt.plot(data_forPlot['x'], data_forPlot['y_lm_grad02'], label = 'Quadratic Model', linestyle=\"--\", color='m')\n",
    "plt.plot(data_forPlot['x'], data_forPlot['y_lm_grad03'], label = 'Qubic Model', linestyle=\"--\", color='b')\n",
    "plt.plot(data_forPlot['x'], data_forPlot['y_lm_grad04'], label = '4th-Order Model', linestyle=\"--\", color='g')\n",
    "\n",
    "plt.legend()\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738dd82-436d-4bf1-86ca-a1cf2254fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "plt.scatter(data_train['x'], lm_grad04_pred_res_train['Residual'], label='Training Data', s=10)\n",
    "plt.legend()\n",
    "\n",
    "plt.legend()\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Residual ($y-\\hat{y}$) of 4th-Order Regression')\n",
    "\n",
    "plt.plot(plt.xlim(), [0, 0], linestyle=\"--\", color='k')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a45064-5594-46a1-bbc4-6e19998311d9",
   "metadata": {},
   "source": [
    "The difference to the qubic regression (both with respect to the model predictions and the residui."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c9e4cf-b728-4794-8cbe-7bab45bf0ae5",
   "metadata": {},
   "source": [
    "## Polynomial Regression with Degree 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d271e1-6b76-4458-afbe-8cbe36971171",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**EXERCISE:** Analoguous to the examples above, do a regression with a polynomial of degree 20. Train and evaluate it. What do you think about it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76fe2bf-4048-417a-b970-84ade3748e17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare data\n",
    "# polynomial_features20 = ...\n",
    "# train_x_poly20 = ...\n",
    "# train_x_poly20_df = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787e5ce0-18ac-4520-ae27-0d0f49dcb809",
   "metadata": {},
   "source": [
    "For a better understanding, let's look at the parameters of the model, and compare them with some parameters we got for models of lower degree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b491418-b507-408b-8841-b079138771bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_coeff_and_intercept(model20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cfcc65-d681-44b2-af0a-d4fd5f0ba4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_coeff_and_intercept(model02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad558337-4cb3-450d-baa5-0fbbacde1cba",
   "metadata": {},
   "source": [
    "The parameters in `model20` are much larger. In addition, the degree of the polynomial is high! $x^{20}$ grows incredibly quickly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45e945e-61e4-4fde-9bab-1ef7314db874",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0.0, 1.0, 2.0, 3.0, 4.0])**20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e8caaa-2757-4fd1-bfe1-84ba3cf58a5c",
   "metadata": {},
   "source": [
    "$4^{20}$ is already over a trillion (one million million). At the same time, ${(1/4)}^{20}$ is less than a trillionth. With such different values, the computer reaches its limits, resulting in so-called *numerical instabilities*. Incidentally, these are also the reason why the regression does not go through all the points exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b398d2-11cf-4ebc-bf4c-5777f1d1b781",
   "metadata": {},
   "source": [
    "## Learning as Generalization: Systematic Performance Comparison on Training and Test Data\n",
    "We now want to look at the performance of models with different degrees on the training and test data. First, we sketch our program in pseudo-code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7fbdd2-20b9-4c12-a895-dda9f0fb2807",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba146986-a071-470b-8059-afbecc30d432",
   "metadata": {},
   "source": [
    "We also write an output function for the root mean square error and the R-squared coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fa4106-1396-4630-8f2b-e09a1980e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse_r2(true_y, pred_y, doPrint=False):\n",
    "    rmse = np.sqrt(mean_squared_error(true_y, pred_y))\n",
    "    r2 = r2_score(true_y, pred_y)\n",
    "    \n",
    "    if doPrint:\n",
    "        print(\"Mean Squared Error (RMSE): \" + str(rmse))\n",
    "        print(\"R-squared: \" + str(r2))\n",
    "        \n",
    "    return [rmse, r2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47293674-5765-4c38-ab1d-48eaebebad55",
   "metadata": {},
   "source": [
    "Now we are ready to implement the systematic performance comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a21ae7-6170-45d2-ba64-af0e3125cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare arrays to store performance results\n",
    "train_performance = np.array([])\n",
    "test_performance = np.array([])\n",
    "max_degree = 10\n",
    "\n",
    "# for different degrees of model - from degree 1 to 10\n",
    "for degree in range(0, max_degree):\n",
    "\n",
    "    # Prepare data: e.g. polynomial features of degree 7\n",
    "    polynomial_features = PolynomialFeatures(degree=degree)\n",
    "    train_x_poly = polynomial_features.fit_transform(train_x)\n",
    "    \n",
    "    # Define model of selected degree\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Train model with training data\n",
    "    model.fit(train_x_poly, train_y)\n",
    "\n",
    "    # Measure R-squared and RMSE (root mean squared error)\n",
    "    # - on training data:\n",
    "    train_y_pred = model.predict(train_x_poly)\n",
    "    train_performance = np.append(train_performance, \n",
    "                                  get_rmse_r2(train_y, train_y_pred),\n",
    "                                  axis=0)\n",
    "    \n",
    "    #  Measure R-squared and RMSE (root mean squared error)\n",
    "    # - on test data:\n",
    "    test_x_poly = polynomial_features.fit_transform(test_x)\n",
    "    test_y_pred = model.predict(test_x_poly)\n",
    "    test_performance = np.append(test_performance,\n",
    "                                 get_rmse_r2(test_y, test_y_pred),\n",
    "                                 axis=0)\n",
    "    \n",
    "train_performance = train_performance.reshape(-1, 2)\n",
    "test_performance = test_performance.reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a024ed-1173-46f4-b955-b5c004ac1d88",
   "metadata": {},
   "source": [
    "With `append(test_performance, get_rmse_r2(test_y, test_y_pred), axis=0)` the new results that we receive via the function `get_rmse_r2(test_y, test_y_pred)` are appended to the existing array.\n",
    "\n",
    "Finally, it is transformed into a two-dimensional structure with `train_performance = train_performance.reshape(-1, 2)` and `test_performance = test_performance.reshape(-1, 2)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6accf53-adb9-4bba-9751-7b4e7a14f70b",
   "metadata": {},
   "source": [
    "Next, we create a graph to plot the mean square error or R-score as a function of the polynomial degree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e40f63-3058-457e-a00f-e9a889f18a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,4))\n",
    "\n",
    "ax[0].plot(range(0, max_degree), train_performance[:, 0], color='b',\n",
    "           label='Training Data')\n",
    "ax[0].plot(range(0, max_degree), test_performance[:, 0], color='g',\n",
    "           label='Test Data')\n",
    "ax[0].grid()\n",
    "ax[0].set_title('RMSE')\n",
    "ax[0].set_xlabel('Degree of Polynomial')\n",
    "\n",
    "ax[1].plot(range(0, max_degree), train_performance[:, 1], color='b',\n",
    "           label='Training Data')\n",
    "ax[1].plot(range(0, max_degree), test_performance[:, 1], color='g',\n",
    "           label='Test Data')\n",
    "ax[1].grid()\n",
    "ax[1].set_title('R-Squared')\n",
    "ax[1].set_xlabel('Degree of Polynomial')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ef313-e17f-4a23-a8c1-f0aece372230",
   "metadata": {},
   "source": [
    "We can thus adapt the model complexity to the test data. With a polynomial of degree 4 we achieve the best quality in terms of both quality measures (RMSE and R-Squared). The appropriate degree for the polynomial regression is therefore 4.\n",
    "\n",
    "**BUT**: we have now determined the model complexity using the test data... and we may also be overfitting! So we need a third, separate data set to determine the final quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3dec5-6a60-468c-ba8b-06e97c2cf3d2",
   "metadata": {},
   "source": [
    "### Crossvalidation\n",
    "We now use crossvalidation to first determine the model complexity and then - for the model of the selected complexity - to be able to make a final statement about the quality of the regression.\n",
    "\n",
    "In the `sklean` library there is a function `cross_validate` which handles the crossvalidation. The function requires\n",
    "* a model that implements the functions `fit()` and `predict`\n",
    "* the independent variables of a training data set\n",
    "* the dependent variables of the training data set\n",
    "* `cv`, the number of runs\n",
    "* `scoring`, the quality measures to be used\n",
    "\n",
    "Details of the function can be found here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html\n",
    "Note that the quality measures are always chosen according to convention so that more is better. R-Square has this property and is offered as `'explained_variance'`. With RMSE, however, less is better; here the negative RMSE is therefore implemented as `'neg_root_mean_squared_error'` as a quality measure. Details: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc6401-4658-4c67-bbe7-05ac321fa36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c30f3d-d5d0-4182-8eb1-61545d68e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=3)\n",
    "train_x_poly = polynomial_features.fit_transform(train_x)\n",
    "\n",
    "model = LinearRegression()\n",
    "cv_result = cross_validate(model, train_x_poly, train_y, cv=5, \n",
    "                           scoring=('explained_variance', 'neg_root_mean_squared_error'),\n",
    "                           return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb7b8b-c640-42ae-a245-b7a6561f2cd8",
   "metadata": {},
   "source": [
    "The resulting `cv_result` contains the metrics for every one of the cross-validation folds - i.e., 5 values in our case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7111e1-8c32-4ccd-a32a-586364f39ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810b2ce2-8a65-40fa-9064-9f2b44827bc8",
   "metadata": {},
   "source": [
    "Technically, the result `cv_result` is a dictionary, where each value is an array with the respective measurements for every cross-validation fold. Hence, we can for example compute the mean and standard deviation over all cross-validation folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a499f0-3830-4fc3-9427-dcc9156bac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, valarr in cv_result.items():\n",
    "    print(name + \": \" + '{:.4}'.format(np.mean(valarr)) + \"+-\" + '{:.4}'.format(np.std(valarr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ba7ddb-868d-408a-a70f-d7b4be45e98c",
   "metadata": {},
   "source": [
    "### Running Cross-Validation for Varying Polynom Degree\n",
    "Next, we run the cross-validation for polynomial regression models with varying degree. We do this once manually, and will then see how we can use predefined Python functionality to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd19b9e-36b6-40bc-bbce-d17f21b5af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_performance_rms_mean = np.zeros(max_degree)\n",
    "train_performance_rms_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee34a0d-a4b5-481c-a64e-3c00c457facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_train_x = train_x\n",
    "overall_train_y = train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf145de-f260-44b0-ad10-9e312b21ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise arrays\n",
    "max_degree = 10\n",
    "\n",
    "train_performance_rms_mean = np.zeros(max_degree+1)\n",
    "train_performance_r2_mean = np.zeros(max_degree+1)\n",
    "train_performance_rms_std = np.zeros(max_degree+1)\n",
    "train_performance_r2_std = np.zeros(max_degree+1)\n",
    "\n",
    "val_performance_rms_mean = np.zeros(max_degree+1)\n",
    "val_performance_r2_mean = np.zeros(max_degree+1)\n",
    "val_performance_rms_std = np.zeros(max_degree+1)\n",
    "val_performance_r2_std = np.zeros(max_degree+1)\n",
    "\n",
    "# for-loop:\n",
    "for degree in range(0, max_degree+1):\n",
    "    # genearte polynomial features \n",
    "    polynomial_features = PolynomialFeatures(degree=degree)\n",
    "    overall_train_x_poly = polynomial_features.fit_transform(overall_train_x)\n",
    "    \n",
    "    # define model and train via cross-validation\n",
    "    model = LinearRegression()\n",
    "    cv_result = cross_validate(model, overall_train_x_poly, overall_train_y, cv=5,\n",
    "                               scoring=('explained_variance', 'neg_root_mean_squared_error'),\n",
    "                               return_train_score=True)\n",
    "\n",
    "    # summarize evaluation on training data\n",
    "    train_performance_rms_mean[degree] = -np.mean(cv_result['train_neg_root_mean_squared_error'])\n",
    "    train_performance_rms_std[degree]  = np.std(-cv_result['train_neg_root_mean_squared_error'])\n",
    "\n",
    "    train_performance_r2_mean[degree] = np.mean(cv_result['train_explained_variance'])\n",
    "    train_performance_r2_std[degree]  = np.std(cv_result['train_explained_variance'])\n",
    "\n",
    "    # summarize evaluation on validation data\n",
    "    val_performance_rms_mean[degree] = -np.mean(cv_result['test_neg_root_mean_squared_error'])\n",
    "    val_performance_rms_std[degree]  =  np.std(-cv_result['test_neg_root_mean_squared_error'])\n",
    "\n",
    "    val_performance_r2_mean[degree] = np.mean(cv_result['test_explained_variance'])\n",
    "    val_performance_r2_std[degree]  = np.std(cv_result['test_explained_variance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2435a1-21cb-41fb-8f40-8c0a87f17616",
   "metadata": {},
   "source": [
    "Let's again plot the performance metrics versus the degree of the polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5180e1-16ca-4d9a-a5db-f7e120485ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,6))\n",
    "\n",
    "ax[0].plot(range(0, max_degree+1), train_performance_rms_mean, color='b', label='Training Data')\n",
    "ax[0].plot(range(0, max_degree+1), val_performance_rms_mean, color='g', label='Validation Data')\n",
    "ax[0].set_title('RMSE')\n",
    "ax[0].set_ylim(0, 15)\n",
    "ax[0].grid()\n",
    "ax[0].set_xlabel('Degree of Polynomial')\n",
    "\n",
    "\n",
    "ax[1].plot(range(0, max_degree+1), train_performance_r2_mean, color='b', label='Training Data')\n",
    "ax[1].plot(range(0, max_degree+1), val_performance_r2_mean, color='g', label='Validation Data')\n",
    "ax[1].set_title('R-Squared')\n",
    "ax[1].set_ylim(0, 1)\n",
    "ax[1].grid()\n",
    "ax[1].set_xlabel('Degree of Polynomial')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ebeaea-9176-49fb-8fc8-5ada74e8f1fa",
   "metadata": {},
   "source": [
    "Note that the curves are now different from the ones we obtained above. This is because we have split the training data used above into training and validation data. Hence, from the original 20 data in `train_x` used above, we now only use 80% for training, and the remaining 20% for evaluation. Hence, the training data consists of only 16 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6426a2a-1905-448c-b441-0df058264eeb",
   "metadata": {},
   "source": [
    "We extend the plot to include the standard deviation of the performance metrics over the 5 cross-validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56107019-ccdf-42fb-a7d1-b6b736c04dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,6))\n",
    "\n",
    "train_performance_rms_mean = np.array(train_performance_rms_mean)\n",
    "train_performance_rms_std = np.array(train_performance_rms_std)\n",
    "\n",
    "ax[0].plot(range(0, max_degree+1), train_performance_rms_mean, color='b', label='Training Data')\n",
    "ax[0].fill_between(range(0, max_degree+1), train_performance_rms_mean-1.96*train_performance_rms_std,\n",
    "                         train_performance_rms_mean+1.96*train_performance_rms_std, color='b', alpha=.15)\n",
    "ax[0].plot(range(0, max_degree+1), val_performance_rms_mean, color='g', label='Validation Data')\n",
    "ax[0].fill_between(range(0, max_degree+1), val_performance_rms_mean-1.96*val_performance_rms_std,\n",
    "                         val_performance_rms_mean+1.96*val_performance_rms_std, color='g', alpha=.15)\n",
    "\n",
    "ax[0].set_title('RMSE')\n",
    "ax[0].set_ylim(0, 15)\n",
    "ax[0].grid()\n",
    "ax[0].set_xlabel('Degree of Polynomial')\n",
    "\n",
    "ax[1].plot(range(0, max_degree+1), train_performance_r2_mean, color='b', label='Training Data')\n",
    "ax[1].fill_between(range(0, max_degree+1), train_performance_r2_mean-1.96*train_performance_r2_std,\n",
    "                         train_performance_r2_mean+1.96*train_performance_r2_std, color='b', alpha=.15)\n",
    "ax[1].plot(range(0, max_degree+1), val_performance_r2_mean, color='g', label='Validation Data')\n",
    "ax[1].fill_between(range(0, max_degree+1), val_performance_r2_mean-1.96*val_performance_r2_std,\n",
    "                         val_performance_r2_mean+1.96*val_performance_r2_std, color='g', alpha=.15)\n",
    "\n",
    "ax[1].set_title('R-Squared')\n",
    "ax[1].set_ylim(-10, 5)\n",
    "ax[1].grid()\n",
    "ax[1].set_xlabel('Degree of Polynomial')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd653f2-c132-43a2-8f2f-ddc2ed921abe",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "As hyperparameter optimization is a very common task, the `scikit-learn` framework offers the `GridSearchCV` class to systematically evaluate a grid of hyperparameters using crossvalidation in order to find the best set of hyperparameters. To make use of the function, one typically defines a so-called `Pipeline`, which comprises all steps including e.g. preprocessing. Also, a grid of all hyperparameters to be evaluated has to be passed; the `fit` function will then determine the best hyperparameter tuple based on a training data set, doing crossvalidation internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa4bff-8b9a-4bb6-91e2-9e2c2acc4d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return Pipeline(steps=[('polyfeat', PolynomialFeatures()), ('regression', LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb43e4-8f2d-4a75-a921-2a806b09f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_grid = GridSearchCV(PolynomialRegression(), param_grid={'polyfeat__degree': range(0, max_degree+1)}, \n",
    "                         cv=10, \n",
    "                         scoring='neg_mean_squared_error', \n",
    "                         verbose=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2e9b7b-e837-4497-9db7-700d4103129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_grid.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4fc041-db5b-4e35-b850-fe4686bb54e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8333da9b-805e-4129-9a94-67fbe520170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0326d1d9-618e-4bbb-a380-fc2109ec04ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1267283-7d35-408d-86ed-c87b5fdee7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_grid.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac362d9-57ea-465d-ac1c-f791d55239fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred = poly_grid.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa07e2d-1c63-47c9-adcf-658625215c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rmse_r2(test_y, test_y_pred, doPrint=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61334f92-9137-4809-a23e-849c6e80859e",
   "metadata": {},
   "source": [
    "## Statistical Model Selection\n",
    "After looking at the prediction performance on new data, let's focus on the statistical model selection, which is based on calculations on the training data.\n",
    "\n",
    "In order to dive deeper into linear Regression, we will use a more specialized library, called `statsmodels`, which we also already imported as `smf`. This library has a class `ols` (for *ordinary least squares*, as we have already seen). We start with a simple linear regression model, aiming to predict `y` based on `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec23d0-59c9-4a3f-a6a0-6e58a86101cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "lm_grad01 = smf.ols('y ~ x', data=data_train)\n",
    "\n",
    "# train / \"fit\" the model\n",
    "lm_grad01_fitted = lm_grad01.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9124cdf3-14c8-4be2-b62b-86cb2d78e05e",
   "metadata": {},
   "source": [
    "The `summary` method gives a comprehensive overview over the model and its performance on the training data (see lecture slides for details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3a6bbb-6d73-401e-827a-fa6430cdea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_grad01_fitted.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdc3738-2e56-410d-97d9-b2124df8caba",
   "metadata": {},
   "source": [
    "We continue with a quadratic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4751534d-9b56-4545-b823-8167e2d08e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['x2'] = data_train['x']**2\n",
    "lm_grad02 = smf.ols('y ~ x + x2 ', data=data_train)\n",
    "lm_grad02_fitted = lm_grad02.fit()\n",
    "\n",
    "lm_grad02_fitted.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1eef76-1fca-4f8e-9998-819edacb9d22",
   "metadata": {},
   "source": [
    "The cubic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e6854c-0587-433a-81da-0d00f25a6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['x3'] = data_train['x']**3\n",
    "lm_grad03 = smf.ols('y ~ x + x2 + x3', data=data_train)\n",
    "lm_grad03_fitted = lm_grad03.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e36215-195b-4ed4-bea1-425421ea82ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_grad03_fitted.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2006da21-2c07-4cd3-be90-b7710e628d67",
   "metadata": {},
   "source": [
    "**Comment**: We see that as we increase the polynomial degree from 1 to 3, all parameters are significant, and the performance (measured e.g. in the R-squared score) increases.\n",
    "\n",
    "Now, let's see what happens if we define a polynomial regression model of degree 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502040e5-659b-404b-ad11-aee3419539c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['x4'] = data_train['x']**4\n",
    "lm_grad04 = smf.ols('y ~ x + x2 + x3 + x4', data=data_train)\n",
    "lm_grad04_fitted = lm_grad04.fit()\n",
    "\n",
    "lm_grad04_fitted.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea73df6b-0032-4c77-a7b8-9e87a73598ee",
   "metadata": {},
   "source": [
    "**Comment**: As discussed in the slides, we see that the coefficient of `x4` is not significant. We will thus omit it, which gets us back to the polynomial regression up to degree 3.\n",
    "\n",
    "Hence, based on the statistical analysis of only the training data, the ordinary least squares fitting was able to identify the correct degree of the polynomial, and detected that the degree-4 contribution is not improving for the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52eeadc-4397-437f-a435-537297732407",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "As a further way to do model selection, we look at regularization. We will use regularisation to determine the best degree for the polynomial regression.\n",
    "\n",
    "First we have to scale the predictor variables. We do so using the `StandardScaler` from `sklearn.preprocessing`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37dc7e0-5a58-484c-bf31-d01a936b8d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly20_scaler = StandardScaler()\n",
    "poly20_scaler = poly20_scaler.fit(train_x_poly20)\n",
    "train_x_poly20_std = poly20_scaler.transform(train_x_poly20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79331aa2-6f64-4950-a66e-ccaaf48865fe",
   "metadata": {},
   "source": [
    "We start with a (randomly chosen) hyperparameter value of 0.1 (note that the hyperparameter is called `alpha` in scikit-learn, unlike in most of the literature):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139f46a-9e40-4414-a731-bc18e5dab08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = Lasso(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d231f26e-5c5f-4357-b0ae-2dccb6b6f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model.fit(train_x_poly20_std, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06932638-80a0-4124-8fec-0ea065d2fbf9",
   "metadata": {},
   "source": [
    "We can get the coefficients. Note that we are doing multiple linear regression, i.e. we have a series of predictor variables - they are increasing powers of a single predictor variable, but the model does not know (nor need to know) about this. The first coefficient is the coefficient to the constant term $x^0$, then follows the coefficient to $x^1$, etc., until $x^20$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ac58d4-1fb9-45c5-8a4e-5973c9875008",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bf132d-4fe7-4c91-a6b1-f542f9b650b7",
   "metadata": {},
   "source": [
    "We see that with this weight for the penalty term, we have several non-zero coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acd55c0-4c21-43d6-bcef-edb7ef851742",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where( np.abs(lasso_model.coef_) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e164391-4982-408b-b6f0-46da227b8637",
   "metadata": {},
   "source": [
    "The coefficients of $x^1$, $x^2$, $x^3$, $x^6$ and $x^19$ are all non-zero.\n",
    "\n",
    "Let's try with a higher weight for the penalty term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26997510-d190-4639-8f86-63d69bb9a1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = Lasso(alpha=0.5, max_iter=100000)\n",
    "lasso_model.fit(train_x_poly20_std, train_y)\n",
    "lasso_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d406e45-a6b5-4971-98fa-14fb5a7ddf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where( np.abs(lasso_model.coef_) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827b41a-6960-40b9-a07d-129e8dcbd5c6",
   "metadata": {},
   "source": [
    "Now, the lasso has indeed found the features that were used to generate the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175feadc-c956-4a2d-b17d-52a66a75ab77",
   "metadata": {},
   "source": [
    "## Grid Search for LASSO Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880755c0-9530-46e4-abef-b95da1517270",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cf681c-8a41-4005-8b64-8cb22adfbc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_range = [0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6858dca-5952-44b2-a8f1-2cc2340d4e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_r2 = GridSearchCV(model, param_grid={'alpha':alpha_range}, scoring ='r2', cv=5)\n",
    "grid_search_r2.fit(train_x_poly20_std, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0131c7-27fd-4c9e-9c72-da0a1daa075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Scoring R2')\n",
    "print('Best R2 score   : ', grid_search_r2.best_score_)\n",
    "print('Best parameters : ', grid_search_r2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93baf952-28a6-4cc2-80d2-4e637d9fed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model_r2 = Lasso(alpha = 2.0)\n",
    "lasso_model_r2.fit(train_x_poly20_std, train_y)\n",
    "lasso_model_r2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b88f46f-3450-41f2-bc10-c4fcc39915cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_mse = GridSearchCV(model, param_grid={'alpha':alpha_range}, scoring ='neg_mean_squared_error', cv=5)\n",
    "grid_search_mse.fit(train_x_poly20_std, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016d850d-33c0-4643-be0a-4699618b78a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Scoring MSE')\n",
    "print('Best MSE score   : ', -grid_search_mse.best_score_)\n",
    "print('Best parameters  : ',  grid_search_mse.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616bbf3-ff9b-4783-86b3-16ccf10f3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model_mse = Lasso(alpha = 0.5)\n",
    "lasso_model_mse.fit(train_x_poly20_std, train_y)\n",
    "lasso_model_mse.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5b7ab4-e9c4-46b9-8c29-10a4c3f556e4",
   "metadata": {},
   "source": [
    "### Impact of the Tuning Parameter\n",
    "Below we illustrate the impact of the tuning parameter $\\alpha$. This is for illustration only, you don't need to understand the code in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c94598e-ac3a-4340-ad24-a473c6ee5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_lasso, coefs_lasso, _ = lasso_path(train_x_poly20_std, train_y, eps=5e-3)\n",
    "\n",
    "plt.figure(figsize = [10, 7])\n",
    "for (degree, coef_l) in enumerate(coefs_lasso.squeeze()):\n",
    "    l1 = plt.semilogx(alphas_lasso, coef_l, label=r'Coefficient of $x^{' + str(degree) + '}$')\n",
    "\n",
    "plt.xlabel(r'Tuning Parameter $\\alpha$')\n",
    "plt.xticks(ticks=[0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10], labels=[0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10])\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.title('Lasso')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72a9288-9303-4e66-a871-9bbf5f7ed30c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
