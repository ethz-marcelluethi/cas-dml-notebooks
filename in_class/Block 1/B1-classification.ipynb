{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "In this notebook, we will see how the classification algorithms, which we have discussed in class, are applied in practice. As in the regression notebook, we will use the `scikit-learn` library to implement the algorithms and `pandas` to manage the data. For visualization, we will `seaborn` and `matplotlib` and for numerical operations, we will use `numpy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparations\n",
    "\n",
    "We start by importing the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loading the data\n",
    "\n",
    "We use [palmerpenguins](https://allisonhorst.github.io/palmerpenguins/articles/intro.html) dataset. This dataset contains data on 344 penguins. The original data includes information on the species, island, bill length, bill depth, flipper length, body mass, sex and year. We have preprocessed the data, removing the missing values and encoding the species as a numerical variable. 0 corresponds to the Adelie species, 1 to the Chinstrap species and 2 to the Gentoo species. Furthermore, we have removed the coluns `island`, `sex` and `year` from the dataset.\n",
    "\n",
    "Let's load the data and take a look at the first few rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin_data = pd.read_csv('./penguins-numeric-all.csv', index_col=0)\n",
    "penguin_data = penguin_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "penguin_data.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by visualizing the data using a pairplot. This plot shows the relationship between the variables in the dataset. The diagonal shows the distribution of each variable. The other plots show the relationship between two variables. The color of the points indicates the species of the penguin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=penguin_data, hue=\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you could only choose one feature to predict the target, which one would you choose?\n",
    "- If you could only choose two features to predict the target, which two would you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "Before applying the classification algorithm, we remove the target variable `species` from the dataset and store it in a variable `y`. The remaining variables are stored in a variable `X`.\n",
    "\n",
    "We can also see from the pairplot that the variables are in different scales. This can be a problem for some algorithms. To solve this problem, we will standardize the variables. This means that we will subtract the mean of each variable and divide by the standard deviation. This way, all variables will have mean 0 and standard deviation 1. This is done using the `StandardScaler` class from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = penguin_data[\"species\"]\n",
    "x = penguin_data.drop(columns=\"species\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_scaled = pd.DataFrame(scaler.fit_transform(x), columns=x.columns, index=x.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with one feature first. We will use the `bill_length_mm` feature to predict the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to add more featuers, just add them to this list, e.g. [\"bill_length_mm\", \"bill_depth_mm\", ...]\n",
    "features = [\"bill_length_mm\"] \n",
    "\n",
    "lr_classifier = LogisticRegression(multi_class=\"multinomial\", max_iter=10000)\n",
    "\n",
    "lr_classifier.fit(x_scaled[features], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain predictions using the `predict` method of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_classifier.predict(x_scaled[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the accuracy. The accuracy is the ratio of the number of correctly classified examples and the total number of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy = accuracy_score(y_pred, y)\n",
    "print(f\"accuracy on training set = {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the accuracy is not great. To understand what kind of errors the model is making, we can compute the confusion matrix. The confusion matrix shows the number of true positives, true negatives, false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the confusion matrix \n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_pred, y)\n",
    "\n",
    "# Step 2: Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises \n",
    "\n",
    "* We see that species 1 is often confused with species 2. Can you explain why this is the case?\n",
    "  * Can you find a feature that better discriminates between species 1 and 2?\n",
    "* Now add a second feature to the model. Which features would you choose? Repeat the training and compare the results.\n",
    "* What is the accuracy of the model when using all features? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the coefficients of the model. The coefficients tell us how much each feature contributes to the prediction. We retrain the model using all features and print the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "lr_classifier.fit(x_scaled[features], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a bar plot with the coefficients of the logistic regression model\n",
    "coefs = lr_classifier.coef_[0]\n",
    "print(coefs)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=coefs, y=features)\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "print(coefs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "- What can you say about the coefficients of the model? Which features are more important for the prediction?\n",
    "- What would happen if we removed the feature with the smallest coefficient? \n",
    "- What would happen if we removed the feature with the largest coefficient? \n",
    "- We use the `StandardScaler` to standardize the features. What would happen if we did not standardize the features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty\n",
    "\n",
    "The logistic regression model gives us a probability that the example belongs to each class. \n",
    "In sci-kit learn, we can obtain these probabilities using the `predict_proba` method. We restrict ourselves to two features for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"bill_length_mm\", \"bill_depth_mm\"]\n",
    "lr_classifier.fit(x_scaled[features], y)\n",
    "probs = lr_classifier.predict_proba(x_scaled[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probs variable contains the probabilities that each example belongs to each class. It is a matrix with 3 columns (for the 3 classes) and one row for each example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the probabilities using a scatter plot. We plot the probability of the example belonging to class 0 on the x-axis and the probability of the example belonging to class 1 on the y-axis. The size of the points indicates how likely it is that the example belongs to the given class. The color of the points indicates the true class of the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probsdf = pd.DataFrame({\"species\": y, \"proba_0\": probs[:, 0], \"proba_1\": probs[:, 1], \"proba_2\": probs[:, 2]})\n",
    "vis_df = pd.concat([x_scaled, probsdf], axis=1)\n",
    "sns.scatterplot(data=vis_df, x=features[0], y=features[1], hue=\"species\", size=\"proba_0\")\n",
    "probsdf.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "- Can you explain why the model is uncertain about some examples?\n",
    "- What can you say about the examples that are far from the decision boundary?\n",
    "- How does the plot look like for the other classes?\n",
    "- How would the plot look like if we used a different pair of features?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A proper workflow for training and evaluating a classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have always used the same data to train and test our model. This is not a good idea. We should always split our data into a training, validation and a test set. As the dataset is relatively small, and logistic regression does not have many hyperparameters, we will not use an explicit validation set, but we will use cross-validation to estimate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# split in train validation and test set\n",
    "train_data, test_data = train_test_split(penguin_data, test_size=0.3, random_state=seed, stratify=penguin_data[\"species\"])\n",
    "\n",
    "y_train = train_data[\"species\"]\n",
    "x_train = train_data.drop(columns=\"species\")\n",
    "y_test = test_data[\"species\"]\n",
    "x_test = test_data.drop(columns=\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we scale the features in order to remove the scale effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_scaled = pd.DataFrame(scaler.transform(x_train), columns=x_train.columns, index=x_train.index)\n",
    "x_test_scaled = pd.DataFrame(scaler.transform(x_test), columns=x_test.columns, index=x_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We restrict ourselves to two features for the rest of the section. This simplifies visualization and allows us later on to plot the decision boundary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cross validation to evalate the models performance\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "features = [\"flipper_length_mm\", \"bill_depth_mm\"]\n",
    "\n",
    "lr_classifier = LogisticRegression(max_iter=10000, C=0.1)\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "cv_results = cross_validate(lr_classifier, x_train_scaled[features], y_train, cv=skf, scoring=[\"accuracy\", \"balanced_accuracy\", \"precision_macro\", \"recall_macro\"])\n",
    "\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "sns.boxplot(data=cv_results_df.drop(columns=[\"fit_time\", \"score_time\"]), orient=\"h\")\n",
    "cv_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "- Experiment with the different features. Which two features would you choose to train the model?\n",
    "- How do you interpret the metrics? Is your model performing well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we evaluate the metrics on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier.fit(x_train_scaled[features], y_train)\n",
    "y_pred_test = lr_classifier.predict(x_test_scaled[features])\n",
    "\n",
    "print(f\"accuracy on test set = {accuracy_score(y_test, y_pred_test)}\")\n",
    "print(f\"balanced accuracy on test set = {balanced_accuracy_score(y_test, y_pred_test)}\")\n",
    "print(f\"precision on test set = {precision_score(y_test, y_pred_test, average='macro')}\")\n",
    "print(f\"recall on test set = {recall_score(y_test, y_pred_test, average='macro')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "- Is the performance on the test set what you would expect from the cross-validation results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we investigate how the performance of the model changes when we adjust the threshold. We will plot the precision-recall curve. The precision is the ratio of the number of true positives and the number of examples classified as positive. The recall is the ratio of the number of true positives and the number of actual positives. The precision-recall curve shows the trade-off between precision and recall for different thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_test = lr_classifier.predict_proba(x_test_scaled[features])[:,1]\n",
    "PrecisionRecallDisplay.from_predictions(y_test[y_test != 2], y_pred_proba_test[y_test != 2], plot_chance_level=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, we also plot the ROC curve. The ROC curve shows the trade-off between the true positive rate and the false positive rate for different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_predictions(y_test[y_test != 2], y_pred_proba_test[y_test != 2], plot_chance_level=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# split in train validation and test set\n",
    "train_data, test_data = train_test_split(penguin_data, test_size=0.3, random_state=seed, stratify=penguin_data[\"species\"])\n",
    "\n",
    "y_train = train_data[\"species\"]\n",
    "x_train = train_data.drop(columns=\"species\")\n",
    "y_test = test_data[\"species\"]\n",
    "x_test = test_data.drop(columns=\"species\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_scaled = pd.DataFrame(scaler.transform(x_train), columns=x_train.columns, index=x_train.index)\n",
    "x_test_scaled = pd.DataFrame(scaler.transform(x_test), columns=x_test.columns, index=x_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a linear classifier. This means that it can only separate the classes using a straight line. We can illustrate this by plotting the decision boundary of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "features = [\"flipper_length_mm\", \"body_mass_g\"]\n",
    "\n",
    "# plot the decision boundary\\\n",
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(x_train_scaled[features], y_train)\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(lr_classifier, x_train_scaled[features], response_method=\"predict\")\n",
    "\n",
    "sns.scatterplot(x_train_scaled, x=features[0], y=features[1], hue=y_train, palette='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a restriction that might prevent the model from capturing the underlying structure of the data. We can overcome this limitation by using non-linear classifiers, such as KNN or decision trees.\n",
    "\n",
    "Let's train a KNN classifier and plot the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# plot the decision boundary\\\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_classifier.fit(x_train_scaled[features], y_train)\n",
    "\n",
    "y_pred_train = knn_classifier.predict(x_train_scaled[features])\n",
    "print(f\"accuracy on training set = {accuracy_score(y_pred_train, y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it classifies all the training data perfectly. Let's check using cross-validation if the model is also able to generalize to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv_results = cross_validate(knn_classifier, x_train_scaled[features], y_train, cv=skf, scoring=[\"balanced_accuracy\", \"f1_macro\"])\n",
    "\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "\n",
    "# create a pointplot to show the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(data=cv_results_df.drop(columns = [\"fit_time\", \"score_time\"]), orient=\"h\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Fold\")\n",
    "plt.title(\"Cross Validation Results\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the decision boundary, we see that the model is very complex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionBoundaryDisplay.from_estimator(knn_classifier, x_train_scaled[features], response_method=\"predict\")\n",
    "sns.scatterplot(x_train_scaled, x=features[0], y=features[1], hue=y_train, palette='viridis')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the model is overfitting. This is because we are using a small number of neighbors. \n",
    "\n",
    "#### Exercise\n",
    "\n",
    "- Experiment with the number of neighbors. What is the best number of neighbors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the best number of neighbors by using cross-validation. We will use the `GridSearchCV` class from `scikit-learn` to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform grid search to find the best hyperparameters\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"n_neighbors\": [1, 3, 5, 7, 9, 11, 13, 15]\n",
    "}\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(model, param_grid, cv=skf, scoring=\"balanced_accuracy\")\n",
    "\n",
    "grid_search.fit(x_train_scaled[features], y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train a decision tree classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dt_classifier = DecisionTreeClassifier(max_depth=3)\n",
    "dt_classifier.fit(x_train_scaled[features], y_train)\n",
    "\n",
    "y_pred_train = dt_classifier.predict(x_train_scaled[features])\n",
    "print(f\"accuracy on training set = {accuracy_score(y_pred_train, y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantages of decision trees is that they are easy to interpret. We can visualize the tree using the `plot_tree` function from `scikit-learn`. In order to clearly see what is happening, we restrict the depth of the tree to a small number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "dsp = plot_tree(dt_classifier, feature_names=features, filled=True, impurity=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also visualize the decision boundary of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionBoundaryDisplay.from_estimator(dt_classifier, x_train_scaled[features], response_method=\"predict\", title='Decision Boundary')\n",
    "sns.scatterplot(x_train_scaled, x=features[0], y=features[1], hue=y_train, palette='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "- Experiment with the depth of the tree. How does the depth influence the decision boundary?\n",
    "- Can you see how the decisions in the tree are reflected in the decision boundary?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question remains what the best depth of the tree is. We can use grid search to find the best hyperparameters, as we did for the KNN classifier.\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "- Can you adapt the code above for KNN to find the best depth for the decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forests and  XGBoost\n",
    "\n",
    "In this last section we quickly show how to train a random forest and an XGBoost classifier.\n",
    "These classifiers are not needed for this dataset, but they are often used on more challenging datasets and we will use them in the project. \n",
    "\n",
    "Let's start with the random forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest classifier works exactly like any other classifier in scikit learn: To train the model, we use the `fit` method. To obtain predictions, we use the `predict` method.\n",
    "\n",
    "As parameters, we can specify the number of trees in the forest and the maximum depth of the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=3)\n",
    "rf_classifier.fit(x_train, y_train)\n",
    "\n",
    "y_pred = rf_classifier.predict(x_test)\n",
    "\n",
    "print(f\"accuracy on test set = {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"balanced accuracy on test set = {balanced_accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn provides a function to plot the feature importances. The feature importances tell us how much each feature contributes to the prediction. The higher the value, the more important the feature is. The computation is based on the impurity decrease of the tree nodes. Details can be found in the [documentation](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html), but are not important for this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the feature importances\n",
    "importances = rf_classifier.feature_importances_\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=importances, y=x_train.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train an XGBoost classifier. XGBoost is a popular algorithm for classification and regression. It is based on decision trees and is known for its high performance.\n",
    "It is not part of the scikit-learn library, but provides a similar interface. We can train the model using the `fit` method and obtain predictions using the `predict` method.\n",
    "\n",
    "XGBoost has many hyperparameters that can be tuned. We will not go into detail here, but we will show how to train the model and how to plot the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier, plot_importance\n",
    "\n",
    "xgb_classifier = XGBClassifier(n_estimators=100, max_depth=3)\n",
    "xgb_classifier.fit(x_train, y_train)\n",
    "\n",
    "y_pred = xgb_classifier.predict(x_test)\n",
    "\n",
    "print(f\"accuracy on test set = {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"balanced accuracy on test set = {balanced_accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the feature importances, we use the `plot_importance` function from the `xgboost` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(xgb_classifier, importance_type='weight')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
